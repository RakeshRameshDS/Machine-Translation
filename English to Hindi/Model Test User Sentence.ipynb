{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"D:/NLP Project/Hindi English/data.p\"\n",
    "X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "l1_vocab_size = len(l1_vocab) + 2 # + <pad>, <ukn>\n",
    "l2_vocab_size = len(l2_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = l2_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == l2_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(l2_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_padding(x, l1_word2idx, length = 20):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [l1_word2idx['<pad>']]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_model(sentences):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # read dataset\n",
    "    dataset_location = \"D:/NLP Project/Hindi English/data.p\"\n",
    "    X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)\n",
    "    \n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # placeholders\n",
    "        encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "        decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "        # output projection\n",
    "        size = 512\n",
    "        w_t = tf.get_variable('proj_w', [l2_vocab_size, size], tf.float32)\n",
    "        b = tf.get_variable('proj_b', [l2_vocab_size], tf.float32)\n",
    "        w = tf.transpose(w_t)\n",
    "        output_projection = (w, b)\n",
    "\n",
    "\n",
    "        # change the model so that output at time t can be fed as input at time t+1\n",
    "        outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                    encoder_inputs,\n",
    "                                                    decoder_inputs,\n",
    "                                                    tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                                    num_encoder_symbols = l1_vocab_size,\n",
    "                                                    num_decoder_symbols = l2_vocab_size,\n",
    "                                                    embedding_size = 80,\n",
    "                                                    feed_previous = True, # <-----this is changed----->\n",
    "                                                    output_projection = output_projection,\n",
    "                                                    dtype = tf.float32)\n",
    "\n",
    "        # ops for projecting outputs\n",
    "        outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "        \n",
    "        sentences = [[l1_word2idx.get(word.strip(',.\" ;:)(|][?!<>'), 0) for word in sentence.split(' ')] for sentence in sentences]\n",
    "        \n",
    "        encoded_sentences = data_padding(sentences, l1_word2idx)\n",
    "        \n",
    "        \n",
    "        # restore all variables - use the last checkpoint saved\n",
    "        saver = tf.train.Saver()\n",
    "        path = tf.train.latest_checkpoint('D:/NLP Project/Hindi English/checkpoints/')\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # restore\n",
    "            saver.restore(sess, path)\n",
    "\n",
    "            # feed data into placeholders\n",
    "            feed = {}\n",
    "            for i in range(input_seq_len):\n",
    "                feed[encoder_inputs[i].name] = np.array([encoded_sentences[j][i] for j in range(len(encoded_sentences))])\n",
    "            feed[decoder_inputs[0].name] = np.array([l2_word2idx['<go>']] * len(encoded_sentences))\n",
    "            \n",
    "            # translate\n",
    "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "            \n",
    "            for i in range(len(encoded_sentences)):\n",
    "                \n",
    "                ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                \n",
    "                #decode output sequence\n",
    "                words = decode_output(ouput_seq)\n",
    "                \n",
    "                temp = \"\"\"\"\"\"\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        temp += words[i] + \" \"\n",
    "                result.append(temp.strip())\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_iitb_dict(IITB_DICTIONARY_FILE_PATH, english_hindi_dict, english_hindi_multiword):\n",
    "    with codecs.open(IITB_DICTIONARY_FILE_PATH, 'r', 'utf-8') as dictionary_file:\n",
    "        for index, line in enumerate(dictionary_file):\n",
    "            end_index = line.find(\"]\")\n",
    "            if(end_index == -1):\n",
    "                # print(line)\n",
    "                continue\n",
    "\n",
    "            hindi_phrase = line[1:end_index]\n",
    "            quote_start_index = line.index('\"')\n",
    "            quote_end_index = line.find('\"', quote_start_index+1)\n",
    "            if(quote_end_index == -1):\n",
    "                # print(line)\n",
    "                continue\n",
    "\n",
    "            english_phrase_with_meaning = line[quote_start_index+1:quote_end_index]\n",
    "            meaning_index = english_phrase_with_meaning.find('(')\n",
    "            if(meaning_index != -1):\n",
    "                english_phrase = english_phrase_with_meaning[:meaning_index]\n",
    "            else:\n",
    "                english_phrase = english_phrase_with_meaning\n",
    "            # print(hindi_phrase, english_phrase)\n",
    "            if(english_phrase.find(' ') == -1):\n",
    "                # Single Word Phrase\n",
    "                english_hindi_dict[english_phrase] = hindi_phrase\n",
    "            else:\n",
    "                english_hindi_multiword[english_phrase] = hindi_phrase\n",
    "\n",
    "            # if(index%100 == 0):\n",
    "            #     print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_hindencorp_dict(HINDENCORP_PATH, english_hindi_dict, english_hindi_multiword):\n",
    "    model = pickle.load(open(HINDENCORP_PATH, 'rb'))\n",
    "    english = model[\"X\"]\n",
    "    hindi = model[\"Y\"]\n",
    "    for index, english_phrase in enumerate(english):\n",
    "        hindi_phrase = hindi[index]\n",
    "        if(english_phrase.find(' ') == -1):\n",
    "            english_hindi_dict[english_phrase] = hindi_phrase\n",
    "        else:\n",
    "            english_hindi_multiword[english_phrase] = hindi_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _read_dictionaries(IITB_DICTIONARY_FILE_PATH, HINDENCORP_PATH, english_hindi_dict, english_hindi_multiword):\n",
    "    _read_iitb_dict(IITB_DICTIONARY_FILE_PATH, english_hindi_dict, english_hindi_multiword)\n",
    "    _read_hindencorp_dict(HINDENCORP_PATH, english_hindi_dict, english_hindi_multiword)\n",
    "\n",
    "    print(\"Dictionary Length:\",len(english_hindi_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _run_on_test_data(english_hindi_dict, TEST_DATA_FILE = \"test_data.p\"):\n",
    "    obj = pickle.load(open(TEST_DATA_FILE, \"rb\"))\n",
    "\n",
    "    absent_words = set()\n",
    "    for keys in obj.keys():\n",
    "        sentence = obj[keys][\"I\"]\n",
    "        obj[keys][\"X\"] = translate_sentence(sentence, english_hindi_dict)\n",
    "        print(obj[keys][\"I\"], obj[keys][\"X\"])\n",
    "\n",
    "    # print(absent_words) \n",
    "    pickle.dump(obj, open('dictionary_test_data.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _translate_sentences(sentences, english_hindi_dict):\n",
    "    translated_sentences = list()\n",
    "    for sentence in sentences:\n",
    "        translated_sentences.append(translate_sentence(sentence, english_hindi_dict))\n",
    "\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, english_hindi_dict):\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    translated_sentence = list()\n",
    "    for word in tokenized:\n",
    "        if word in english_hindi_dict:\n",
    "            translated_sentence.append(english_hindi_dict[word])\n",
    "        elif not word.isalnum():\n",
    "            translated_sentence.append(word)\n",
    "        # else:\n",
    "            # absent_words.add(word)\n",
    "\n",
    "    return ' '.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary_translation(sentences, IITB_DICTIONARY_FILE_PATH = 'D:/NLP Project/Hindi English/rahul/UW-Hindi_Dict-20131003.txt', \n",
    "                           HINDENCORP_PATH = 'D:/NLP Project/Hindi English/hindencorp05.p'):\n",
    "\n",
    "    english_hindi_dict = dict()\n",
    "    english_hindi_multiword = dict()\n",
    "\n",
    "    _read_dictionaries(IITB_DICTIONARY_FILE_PATH, HINDENCORP_PATH, english_hindi_dict, english_hindi_multiword)\n",
    "    # _run_on_test_data(english_hindi_dict)\n",
    "    return _translate_sentences(sentences, english_hindi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Length: 48903\n",
      "==================================\n",
      "Input -  there should be only one domain object\n",
      "Actual -  एक ही <ukn> वस्तु होना चाहिए \n",
      "Baseline -  है। गा,गे,गीईना रहना केवल एक डोमेन वस्तु\n",
      "Predicted -  केवल एक डोमेन को ऑब्जेक्ट होना चाहिए\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  max <ukn>\n",
      "Actual -  मैक्स पेन\n",
      "Baseline -  अधिकतम < >\n",
      "Predicted -  अधिकतम <ukn>\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  video playback\n",
      "Actual -  वीडियो प्लेबैक\n",
      "Baseline -  वीडियो प्लेबैक\n",
      "Predicted -  वीडियो प्लेबैक\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  unknown time remaining\n",
      "Actual -  अज्ञात समय शेष\n",
      "Baseline -  अज्ञात समय(m) शेष\n",
      "Predicted -  अनजान समय शेष\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  pause\n",
      "Actual -  ठहरें\n",
      "Baseline -  ठहराना\n",
      "Predicted -  रोकें\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  enter password to unlock your login keyring\n",
      "Actual -  लॉगिन कीरिंग को खोलने के लिये कूटशब्द डालें\n",
      "Baseline -  प्रविष्ट करें पासवर्डों प्रति खोलें तुम्हारा लॉगिन करें\n",
      "Predicted -  आपका लॉगिन कीरिंग को पासवर्ड के लिए कूटशब्द दाखिल करें\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  1990 asian winter games\n",
      "Actual -  1990 एशियाई शीतकालीन खेल\n",
      "Baseline -  शीतकाल खेल\n",
      "Predicted -  1990 एशियाई शीतकालीन खेलों\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  don't import if label set already exists\n",
      "Actual -  आयात अगर लेबल सेट पहले से मौजूद न करें\n",
      "Baseline -  काफ़ी होना n't आयात करें (m) कि लेबल सेट करें एकदम उपस्थित है\n",
      "Predicted -  लेबल लेबल सेट मौजूद स्थान पर मौजूद नहीं है\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  it is in south central nepal and spread across 932 square kilometers\n",
      "Actual -  <ukn> वर्ग किमी में फैला यह उद्यान दक्षिण मध्य नेपाल में स्थित है।\n",
      "Baseline -  है अंदर (i) दक्षिण canada/केन्द्रीय नेपालकी और प्रचार तिरछे ढंग से वर्ग\n",
      "Predicted -  दक्षिण नेपाल और दक्षिण अमेरिकी प्रकाश <ukn> और फैला <ukn>\n",
      "----------------------------------\n",
      "==================================\n",
      "Input -  a good reputation is more valuable than money\n",
      "Actual -  बेहतर प्रतिष्ठा पैसे से कहीं अधिक मूल्यवान होती है\n",
      "Baseline -  सही नाम है आर्थिक अनमोल स् धन\n",
      "Predicted -  पैसे की पैसे सबसे अधिक पैसे होती है।\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "input_file = codecs.open(\"D:/NLP Project/Hindi English/test/input.txt\", encoding = \"utf-8\", mode=\"r\")\n",
    "output_file = codecs.open(\"D:/NLP Project/Hindi English/test/output.txt\", encoding=\"utf-8\", mode=\"r\")\n",
    "\n",
    "inputs = input_file.read().split('\\n')\n",
    "actual_output = output_file.read().split('\\n')\n",
    "\n",
    "def attention_nn_function(x):\n",
    "    return translate_model(x)\n",
    "\n",
    "def baseline_dict_function(x):\n",
    "    return dictionary_translation(x)\n",
    "\n",
    "predicted_output = attention_nn_function(inputs)\n",
    "baseline_output = baseline_dict_function(inputs)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "d = {'input':inputs,'actual_output':actual_output,'predicted_output':predicted_output,'baseline_output':baseline_output}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    print('==================================')\n",
    "    print('Input - ', inputs[i])\n",
    "    print('Actual - ', actual_output[i])\n",
    "    print('Baseline - ', baseline_output[i])\n",
    "    print('Predicted - ', predicted_output[i])\n",
    "    print('----------------------------------')\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['आपका नाम क्या है', 'कौन कौन है।', 'नही', 'बुरा', 'बुरा नहीं', 'जल जीवनी है', 'आप क्या कर सकते हैं', 'दिनांक दिनांक क्या है', 'मृत जानवर', 'आज आपको कैसे हो', 'मुझे विश्वास का एक अच्छा मजाक', 'मुझे दो पागल बनाएं', 'कौन आपकी पिता है।']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"what is your name\", \"who is this\", \"not\", \"bad\", \"not bad\", \"water is life\", \"what do you do\", \n",
    "            \"what is todays date\", \"dead animal\", \"how are you today\", \"tell me a good joke\", \"dont make me mad\", \"who is your dad\"]\n",
    "result = translate_model(sentences)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

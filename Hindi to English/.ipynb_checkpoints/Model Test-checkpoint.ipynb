{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"./data.p\"\n",
    "X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "l1_vocab_size = len(l1_vocab) + 2 # + <pad>, <ukn>\n",
    "l2_vocab_size = len(l2_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sentence_string(sentences, idx2word):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        temp = \"\"\"\"\"\"\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in [1, 2, 3]:\n",
    "                temp += idx2word[sentence[i]] +\" \"\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = l2_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == l2_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(l2_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_test_data = pickle.load(open(\"./encoded_test.p\", \"rb\"))\n",
    "l1_test_data = whole_test_data['X']\n",
    "l2_test_data = whole_test_data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  1  Processed\n",
      "Batch -  2  Processed\n",
      "Batch -  3  Processed\n",
      "Batch -  4  Processed\n",
      "Batch -  5  Processed\n",
      "Batch -  6  Processed\n",
      "Batch -  7  Processed\n",
      "Batch -  8  Processed\n",
      "Batch -  9  Processed\n",
      "Batch -  10  Processed\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# For whole test data\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [l2_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [l2_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = l1_vocab_size,\n",
    "                                                num_decoder_symbols = l2_vocab_size,\n",
    "                                                embedding_size = 80,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "    \n",
    "    for idx in range(1, 11):\n",
    "        print(\"Batch - \",idx, \" Processed\")\n",
    "        \n",
    "        fp = codecs.open(\"./test_results/test_result_\"+str(idx)+\".txt\", encoding=\"utf-8\", mode=\"w\")\n",
    "        \n",
    "        l1_sentences_encoded = l1_test_data[(idx-1)*1000:idx*1000]\n",
    "        l2_sentences_encoded = l2_test_data[(idx-1)*1000:idx*1000]\n",
    "        \n",
    "        l1_sentences = decode_sentence_string(l1_sentences_encoded, l1_idx2word)\n",
    "        l2_sentences = decode_sentence_string(l2_sentences_encoded, l2_idx2word)\n",
    "        \n",
    "        # restore all variables - use the last checkpoint saved\n",
    "        saver = tf.train.Saver()\n",
    "        path = tf.train.latest_checkpoint('./checkpoints/')\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # restore\n",
    "            saver.restore(sess, path)\n",
    "\n",
    "            # feed data into placeholders\n",
    "            feed = {}\n",
    "            for i in range(input_seq_len):\n",
    "                feed[encoder_inputs[i].name] = np.array([l1_sentences_encoded[j][i] for j in range(len(l1_sentences_encoded))])\n",
    "\n",
    "            feed[decoder_inputs[0].name] = np.array([l2_word2idx['<go>']] * len(l1_sentences_encoded))\n",
    "\n",
    "            # translate\n",
    "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "\n",
    "            # decode seq.\n",
    "            for i in range(len(l1_sentences_encoded)):\n",
    "                fp.write('\\n')\n",
    "                fp.write('{}.\\n--------------------------------'.format(i+1))\n",
    "                fp.write('\\n')\n",
    "                ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                \n",
    "                #decode output sequence\n",
    "                words = decode_output(ouput_seq)\n",
    "\n",
    "                fp.write('Input\\t\\t - ')\n",
    "                fp.write(l1_sentences[i])\n",
    "                fp.write('\\n')\n",
    "\n",
    "                fp.write('Actual\\t\\t - ')\n",
    "                fp.write(l2_sentences[i])\n",
    "                fp.write('\\n')\n",
    "\n",
    "                fp.write('Predicted\\t\\t - ')\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        #print(words[i], end=' ')\n",
    "                        fp.write(words[i]+\" \")\n",
    "                fp.write('\\n--------------------------------')\n",
    "                fp.write('\\n')\n",
    "        fp.close()\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

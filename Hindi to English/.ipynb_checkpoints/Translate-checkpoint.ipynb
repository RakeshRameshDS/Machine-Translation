{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"D:/NLP Project/English from Hindi/data.p\"\n",
    "X, Y, en_word2idx, en_idx2word, en_vocab, hi_word2idx, hi_idx2word, hi_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for', 'it', 'this', 'on', 'you', 'was', 'as', 'are', 'with', 'be', 'not', 'by', 'or', 'he', 'from', 'his', 'have', 'but', 'at', 'an', 'which']\n",
      "['के', '', 'में', 'है', 'की', 'और', 'से', 'को', 'का', 'हैं', 'कि', 'पर', 'एक', 'नहीं', 'लिए', 'यह', 'भी', 'इस', 'कर', 'ने', 'हो', '।', 'ही', 'करने', 'जो', 'तो', 'किया', 'था', 'या', 'आप']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:30])\n",
    "print(hi_vocab[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence encoded in English [2386, 62, 21, 27, 1592, 6, 62, 60, 546, 6, 20, 287]\n",
      "Sentence encoded in Hindi [13589, 4, 125, 28, 140, 63, 104, 37, 27, 14, 365, 17, 7, 5]\n",
      "Decoded:\n",
      "------------------------\n",
      "politicians do not have permission to do what needs to be done \n",
      "\n",
      "राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करने कि अनुमति नहीं है  \n",
      "\n",
      "Total dataset size 147487\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence encoded in English\", X[1])\n",
    "print(\"Sentence encoded in Hindi\", Y[1])\n",
    "\n",
    "print('Decoded:\\n------------------------')\n",
    "\n",
    "for i in range(len(X[1])):\n",
    "    print(en_idx2word[X[1][i]], end=' ')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(Y[1])):\n",
    "    print(hi_idx2word[Y[1][i]], end = ' ')\n",
    "    \n",
    "print('\\n')\n",
    "print(\"Total dataset size\", len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 20):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_word2idx['<pad>']]\n",
    "        y[i] = [hi_word2idx['<go>']] + y[i] + [hi_word2idx['<eos>']] + (length-len(y[i])) * [hi_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "hi_vocab_size = len(hi_vocab) + 2 # + <pad>, <ukn>\n",
    "en_vocab_size = len(en_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{0}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [hi_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [hi_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = hi_vocab_size,\n",
    "                                            embedding_size = 80,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(logits, labels):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = hi_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.nn.seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = hi_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == hi_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(hi_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 3e-3\n",
    "batch_size = 8\n",
    "steps = 30000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.99).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 9.108528137207031\n",
      "step: 49, loss: 6.4602460861206055\n",
      "step: 99, loss: 5.652222156524658\n",
      "step: 149, loss: 5.35828971862793\n",
      "step: 199, loss: 5.273741245269775\n",
      "step: 249, loss: 5.040556907653809\n",
      "step: 299, loss: 4.656556129455566\n",
      "step: 349, loss: 4.758871555328369\n",
      "step: 399, loss: 4.582771301269531\n",
      "step: 449, loss: 4.413110733032227\n",
      "step: 499, loss: 4.672544479370117\n",
      "step: 549, loss: 4.44556188583374\n",
      "step: 599, loss: 4.5256667137146\n",
      "step: 649, loss: 4.011312961578369\n",
      "step: 699, loss: 4.229057312011719\n",
      "step: 749, loss: 4.0585198402404785\n",
      "step: 799, loss: 4.519129753112793\n",
      "step: 849, loss: 3.850048542022705\n",
      "step: 899, loss: 4.152567386627197\n",
      "step: 949, loss: 4.036608695983887\n",
      "step: 999, loss: 3.4826040267944336\n",
      "Checkpoint is saved\n",
      "step: 1049, loss: 3.1768641471862793\n",
      "step: 1099, loss: 3.5659308433532715\n",
      "step: 1149, loss: 3.3296008110046387\n",
      "step: 1199, loss: 3.23958158493042\n",
      "step: 1249, loss: 3.35125732421875\n",
      "step: 1299, loss: 3.204448699951172\n",
      "step: 1349, loss: 2.8863306045532227\n",
      "step: 1399, loss: 3.323901891708374\n",
      "step: 1449, loss: 3.3216261863708496\n",
      "step: 1499, loss: 2.8467488288879395\n",
      "step: 1549, loss: 2.6328067779541016\n",
      "step: 1599, loss: 3.1027071475982666\n",
      "step: 1649, loss: 2.351844549179077\n",
      "step: 1699, loss: 2.741598606109619\n",
      "step: 1749, loss: 2.800858974456787\n",
      "step: 1799, loss: 2.792921543121338\n",
      "step: 1849, loss: 3.293583869934082\n",
      "step: 1899, loss: 2.740015983581543\n",
      "step: 1949, loss: 2.5635886192321777\n",
      "step: 1999, loss: 2.3993263244628906\n",
      "Checkpoint is saved\n",
      "step: 2049, loss: 2.4653706550598145\n",
      "step: 2099, loss: 2.1334214210510254\n",
      "step: 2149, loss: 2.689319133758545\n",
      "step: 2199, loss: 2.295189380645752\n",
      "step: 2249, loss: 2.247509002685547\n",
      "step: 2299, loss: 2.2829153537750244\n",
      "step: 2349, loss: 1.8988168239593506\n",
      "step: 2399, loss: 2.1967897415161133\n",
      "step: 2449, loss: 1.771716833114624\n",
      "step: 2499, loss: 2.38106632232666\n",
      "step: 2549, loss: 2.6966500282287598\n",
      "step: 2599, loss: 2.2394731044769287\n",
      "step: 2649, loss: 2.0011587142944336\n",
      "step: 2699, loss: 1.889173984527588\n",
      "step: 2749, loss: 1.9914958477020264\n",
      "step: 2799, loss: 2.0157694816589355\n",
      "step: 2849, loss: 2.0993075370788574\n",
      "step: 2899, loss: 2.153921365737915\n",
      "step: 2949, loss: 2.3154101371765137\n",
      "step: 2999, loss: 2.073124885559082\n",
      "Checkpoint is saved\n",
      "step: 3049, loss: 1.726961374282837\n",
      "step: 3099, loss: 2.031088352203369\n",
      "step: 3149, loss: 1.643930435180664\n",
      "step: 3199, loss: 1.9262405633926392\n",
      "step: 3249, loss: 1.8435463905334473\n",
      "step: 3299, loss: 1.8786468505859375\n",
      "step: 3349, loss: 1.9696508646011353\n",
      "step: 3399, loss: 1.8901721239089966\n",
      "step: 3449, loss: 1.6659163236618042\n",
      "step: 3499, loss: 1.7745094299316406\n",
      "step: 3549, loss: 1.7145131826400757\n",
      "step: 3599, loss: 1.529242753982544\n",
      "step: 3649, loss: 1.6151106357574463\n",
      "step: 3699, loss: 1.350841760635376\n",
      "step: 3749, loss: 1.592245101928711\n",
      "step: 3799, loss: 1.5804693698883057\n",
      "step: 3849, loss: 1.4956724643707275\n",
      "step: 3899, loss: 1.6663928031921387\n",
      "step: 3949, loss: 1.5819305181503296\n",
      "step: 3999, loss: 1.490748405456543\n",
      "Checkpoint is saved\n",
      "step: 4049, loss: 1.8843162059783936\n",
      "step: 4099, loss: 1.3992818593978882\n",
      "step: 4149, loss: 1.6325008869171143\n",
      "step: 4199, loss: 1.243052363395691\n",
      "step: 4249, loss: 1.9448268413543701\n",
      "step: 4299, loss: 1.4877219200134277\n",
      "step: 4349, loss: 1.3105186223983765\n",
      "step: 4399, loss: 1.7364890575408936\n",
      "step: 4449, loss: 1.598136305809021\n",
      "step: 4499, loss: 2.7697336673736572\n",
      "step: 4549, loss: 1.3793315887451172\n",
      "step: 4599, loss: 1.7065669298171997\n",
      "step: 4649, loss: 1.712294101715088\n",
      "step: 4699, loss: 1.5053882598876953\n",
      "step: 4749, loss: 1.347030758857727\n",
      "step: 4799, loss: 1.437086820602417\n",
      "step: 4849, loss: 1.554246425628662\n",
      "step: 4899, loss: 1.3457789421081543\n",
      "step: 4949, loss: 1.390721082687378\n",
      "step: 4999, loss: 1.58487868309021\n",
      "Checkpoint is saved\n",
      "step: 5049, loss: 1.4402283430099487\n",
      "step: 5099, loss: 1.4109874963760376\n",
      "step: 5149, loss: 1.3020137548446655\n",
      "step: 5199, loss: 1.3120248317718506\n",
      "step: 5249, loss: 1.5310137271881104\n",
      "step: 5299, loss: 1.3822121620178223\n",
      "step: 5349, loss: 1.2329779863357544\n",
      "step: 5399, loss: 1.4728760719299316\n",
      "step: 5449, loss: 1.2270231246948242\n",
      "step: 5499, loss: 1.258460521697998\n",
      "step: 5549, loss: 1.3433399200439453\n",
      "step: 5599, loss: 1.284438133239746\n",
      "step: 5649, loss: 1.1087735891342163\n",
      "step: 5699, loss: 1.2696691751480103\n",
      "step: 5749, loss: 0.9866934418678284\n",
      "step: 5799, loss: 1.1946628093719482\n",
      "step: 5849, loss: 1.2647565603256226\n",
      "step: 5899, loss: 1.149254322052002\n",
      "step: 5949, loss: 1.0430361032485962\n",
      "step: 5999, loss: 1.0651514530181885\n",
      "Checkpoint is saved\n",
      "step: 6049, loss: 1.2512009143829346\n",
      "step: 6099, loss: 1.242361307144165\n",
      "step: 6149, loss: 1.3256580829620361\n",
      "step: 6199, loss: 1.1574373245239258\n",
      "step: 6249, loss: 1.0557929277420044\n",
      "step: 6299, loss: 1.0730172395706177\n",
      "step: 6349, loss: 1.2752294540405273\n",
      "step: 6399, loss: 1.2438969612121582\n",
      "step: 6449, loss: 1.0628774166107178\n",
      "step: 6499, loss: 1.10112464427948\n",
      "step: 6549, loss: 1.0388925075531006\n",
      "step: 6599, loss: 1.2517725229263306\n",
      "step: 6649, loss: 1.2498120069503784\n",
      "step: 6699, loss: 0.8832746148109436\n",
      "step: 6749, loss: 0.9449650049209595\n",
      "step: 6799, loss: 1.2192258834838867\n",
      "step: 6849, loss: 1.2501574754714966\n",
      "step: 6899, loss: 0.8640416860580444\n",
      "step: 6949, loss: 0.875524640083313\n",
      "step: 6999, loss: 1.0248935222625732\n",
      "Checkpoint is saved\n",
      "step: 7049, loss: 1.0853610038757324\n",
      "step: 7099, loss: 0.9718332290649414\n",
      "step: 7149, loss: 0.9086577892303467\n",
      "step: 7199, loss: 0.8052763938903809\n",
      "step: 7249, loss: 0.8363127708435059\n",
      "step: 7299, loss: 1.04146409034729\n",
      "step: 7349, loss: 1.192649483680725\n",
      "step: 7399, loss: 0.9317744970321655\n",
      "step: 7449, loss: 0.8859889507293701\n",
      "step: 7499, loss: 1.0357966423034668\n",
      "step: 7549, loss: 0.8855143785476685\n",
      "step: 7599, loss: 1.0149381160736084\n",
      "step: 7649, loss: 0.9298408627510071\n",
      "step: 7699, loss: 0.9263092279434204\n",
      "step: 7749, loss: 0.908103346824646\n",
      "step: 7799, loss: 0.9186011552810669\n",
      "step: 7849, loss: 0.725623369216919\n",
      "step: 7899, loss: 0.9592430591583252\n",
      "step: 7949, loss: 0.8510800004005432\n",
      "step: 7999, loss: 0.8596799969673157\n",
      "Checkpoint is saved\n",
      "step: 8049, loss: 0.9062795639038086\n",
      "step: 8099, loss: 0.9724711179733276\n",
      "step: 8149, loss: 0.725248396396637\n",
      "step: 8199, loss: 0.9329072833061218\n",
      "step: 8249, loss: 0.9879291653633118\n",
      "step: 8299, loss: 0.7801531553268433\n",
      "step: 8349, loss: 1.0334975719451904\n",
      "step: 8399, loss: 0.8440600633621216\n",
      "step: 8449, loss: 0.8463830947875977\n",
      "step: 8499, loss: 0.8075389862060547\n",
      "step: 8549, loss: 0.798155665397644\n",
      "step: 8599, loss: 1.0801470279693604\n",
      "step: 8649, loss: 0.8374383449554443\n",
      "step: 8699, loss: 0.9616175889968872\n",
      "step: 8749, loss: 0.8049555420875549\n",
      "step: 8799, loss: 1.0032172203063965\n",
      "step: 8849, loss: 0.900344729423523\n",
      "step: 8899, loss: 0.727695107460022\n",
      "step: 8949, loss: 0.6803426146507263\n",
      "step: 8999, loss: 0.8110238313674927\n",
      "Checkpoint is saved\n",
      "step: 9049, loss: 1.2520112991333008\n",
      "step: 9099, loss: 0.9693022966384888\n",
      "step: 9149, loss: 0.9834931492805481\n",
      "step: 9199, loss: 1.0047335624694824\n",
      "step: 9249, loss: 0.8714140057563782\n",
      "step: 9299, loss: 0.6858433485031128\n",
      "step: 9349, loss: 0.7824349999427795\n",
      "step: 9399, loss: 0.8631249666213989\n",
      "step: 9449, loss: 0.6994345784187317\n",
      "step: 9499, loss: 0.6609863638877869\n",
      "step: 9549, loss: 0.7499527931213379\n",
      "step: 9599, loss: 0.8528387546539307\n",
      "step: 9649, loss: 0.8676502704620361\n",
      "step: 9699, loss: 0.9147567749023438\n",
      "step: 9749, loss: 0.6429058313369751\n",
      "step: 9799, loss: 0.47252246737480164\n",
      "step: 9849, loss: 0.8205362558364868\n",
      "step: 9899, loss: 0.935060441493988\n",
      "step: 9949, loss: 0.6749640107154846\n",
      "step: 9999, loss: 0.8230071663856506\n",
      "Checkpoint is saved\n",
      "step: 10049, loss: 0.706281304359436\n",
      "step: 10099, loss: 0.7629712224006653\n",
      "step: 10149, loss: 0.7992517948150635\n",
      "step: 10199, loss: 0.6290880441665649\n",
      "step: 10249, loss: 0.7109321355819702\n",
      "step: 10299, loss: 0.6770893931388855\n",
      "step: 10349, loss: 0.6410946249961853\n",
      "step: 10399, loss: 0.8324373960494995\n",
      "step: 10449, loss: 0.735998272895813\n",
      "step: 10499, loss: 0.6608785390853882\n",
      "step: 10549, loss: 0.8287129998207092\n",
      "step: 10599, loss: 0.6420267820358276\n",
      "step: 10649, loss: 0.73582923412323\n",
      "step: 10699, loss: 0.654699981212616\n",
      "step: 10749, loss: 0.7376910448074341\n",
      "step: 10799, loss: 0.8397854566574097\n",
      "step: 10849, loss: 0.6627386212348938\n",
      "step: 10899, loss: 0.662339448928833\n",
      "step: 10949, loss: 0.7507731318473816\n",
      "step: 10999, loss: 0.7231370210647583\n",
      "Checkpoint is saved\n",
      "step: 11049, loss: 0.7447367310523987\n",
      "step: 11099, loss: 0.6387052536010742\n",
      "step: 11149, loss: 0.5621563792228699\n",
      "step: 11199, loss: 0.6799488067626953\n",
      "step: 11249, loss: 0.6049709320068359\n",
      "step: 11299, loss: 0.8001764416694641\n",
      "step: 11349, loss: 0.6925191879272461\n",
      "step: 11399, loss: 0.707676887512207\n",
      "step: 11449, loss: 0.7571991682052612\n",
      "step: 11499, loss: 0.557022213935852\n",
      "step: 11549, loss: 0.5169013142585754\n",
      "step: 11599, loss: 0.6954565048217773\n",
      "step: 11649, loss: 0.7415287494659424\n",
      "step: 11699, loss: 0.6869157552719116\n",
      "step: 11749, loss: 0.7060448527336121\n",
      "step: 11799, loss: 0.801510214805603\n",
      "step: 11849, loss: 0.8079546689987183\n",
      "step: 11899, loss: 0.541041374206543\n",
      "step: 11949, loss: 0.6812758445739746\n",
      "step: 11999, loss: 0.6291427612304688\n",
      "Checkpoint is saved\n",
      "step: 12049, loss: 0.5714769959449768\n",
      "step: 12099, loss: 0.7069364190101624\n",
      "step: 12149, loss: 0.6177634596824646\n",
      "step: 12199, loss: 0.5122430324554443\n",
      "step: 12249, loss: 0.6384451985359192\n",
      "step: 12299, loss: 0.6562645435333252\n",
      "step: 12349, loss: 0.599459707736969\n",
      "step: 12399, loss: 0.649074912071228\n",
      "step: 12449, loss: 0.6060806512832642\n",
      "step: 12499, loss: 0.5458678007125854\n",
      "step: 12549, loss: 0.5755611658096313\n",
      "step: 12599, loss: 0.6452069282531738\n",
      "step: 12649, loss: 0.6579961180686951\n",
      "step: 12699, loss: 0.7033372521400452\n",
      "step: 12749, loss: 0.5304564237594604\n",
      "step: 12799, loss: 0.6302230954170227\n",
      "step: 12849, loss: 0.6376757025718689\n",
      "step: 12899, loss: 0.485024631023407\n",
      "step: 12949, loss: 0.6282303333282471\n",
      "step: 12999, loss: 0.610832691192627\n",
      "Checkpoint is saved\n",
      "step: 13049, loss: 0.687112033367157\n",
      "step: 13099, loss: 0.43299412727355957\n",
      "step: 13149, loss: 0.6936792135238647\n",
      "step: 13199, loss: 0.4182840585708618\n",
      "step: 13249, loss: 0.4806910455226898\n",
      "step: 13299, loss: 0.5085066556930542\n",
      "step: 13349, loss: 0.5149083137512207\n",
      "step: 13399, loss: 0.47975122928619385\n",
      "step: 13449, loss: 0.5999757051467896\n",
      "step: 13499, loss: 0.4974369704723358\n",
      "step: 13549, loss: 0.6957201957702637\n",
      "step: 13599, loss: 0.5647146701812744\n",
      "step: 13649, loss: 0.550739049911499\n",
      "step: 13699, loss: 0.446636438369751\n",
      "step: 13749, loss: 0.5540275573730469\n",
      "step: 13799, loss: 0.42471402883529663\n",
      "step: 13849, loss: 0.5864197015762329\n",
      "step: 13899, loss: 0.7709967494010925\n",
      "step: 13949, loss: 0.5525299906730652\n",
      "step: 13999, loss: 0.5973286032676697\n",
      "Checkpoint is saved\n",
      "step: 14049, loss: 0.6239705085754395\n",
      "step: 14099, loss: 0.38291794061660767\n",
      "step: 14149, loss: 0.4559576213359833\n",
      "step: 14199, loss: 0.6371376514434814\n",
      "step: 14249, loss: 0.6047989726066589\n",
      "step: 14299, loss: 0.4670031666755676\n",
      "step: 14349, loss: 0.668295681476593\n",
      "step: 14399, loss: 0.4947311580181122\n",
      "step: 14449, loss: 0.571092426776886\n",
      "step: 14499, loss: 0.5027998685836792\n",
      "step: 14549, loss: 0.6307145357131958\n",
      "step: 14599, loss: 0.5975121259689331\n",
      "step: 14649, loss: 0.39921343326568604\n",
      "step: 14699, loss: 0.47029411792755127\n",
      "step: 14749, loss: 0.5243041515350342\n",
      "step: 14799, loss: 0.4893784821033478\n",
      "step: 14849, loss: 0.4869445860385895\n",
      "step: 14899, loss: 0.5089306831359863\n",
      "step: 14949, loss: 0.4764450192451477\n",
      "step: 14999, loss: 0.336948037147522\n",
      "Checkpoint is saved\n",
      "step: 15049, loss: 0.5880148410797119\n",
      "step: 15099, loss: 0.3935137987136841\n",
      "step: 15149, loss: 0.6782194375991821\n",
      "step: 15199, loss: 0.5677422285079956\n",
      "step: 15249, loss: 0.36933571100234985\n",
      "step: 15299, loss: 0.4957585334777832\n",
      "step: 15349, loss: 0.3156437575817108\n",
      "step: 15399, loss: 0.4987374246120453\n",
      "step: 15449, loss: 0.5673002600669861\n",
      "step: 15499, loss: 0.6048098802566528\n",
      "step: 15549, loss: 0.36143696308135986\n",
      "step: 15599, loss: 0.6003842353820801\n",
      "step: 15649, loss: 0.3021891117095947\n",
      "step: 15699, loss: 0.5292735695838928\n",
      "step: 15749, loss: 0.4062687158584595\n",
      "step: 15799, loss: 0.3928643465042114\n",
      "step: 15849, loss: 0.4193168878555298\n",
      "step: 15899, loss: 0.43280789256095886\n",
      "step: 15949, loss: 0.4809190630912781\n",
      "step: 15999, loss: 0.46666160225868225\n",
      "Checkpoint is saved\n",
      "step: 16049, loss: 0.5953830480575562\n",
      "step: 16099, loss: 0.5831452012062073\n",
      "step: 16149, loss: 0.4198533892631531\n",
      "step: 16199, loss: 0.46301335096359253\n",
      "step: 16249, loss: 0.41578543186187744\n",
      "step: 16299, loss: 0.6451691389083862\n",
      "step: 16349, loss: 0.4696478247642517\n",
      "step: 16399, loss: 0.5523744821548462\n",
      "step: 16449, loss: 0.34028345346450806\n",
      "step: 16499, loss: 0.5004279613494873\n",
      "step: 16549, loss: 0.4747219681739807\n",
      "step: 16599, loss: 0.7245829105377197\n",
      "step: 16649, loss: 0.47670549154281616\n",
      "step: 16699, loss: 0.429818719625473\n",
      "step: 16749, loss: 0.3608774244785309\n",
      "step: 16799, loss: 0.44727256894111633\n",
      "step: 16849, loss: 0.3512954115867615\n",
      "step: 16899, loss: 0.5575308203697205\n",
      "step: 16949, loss: 0.3966759443283081\n",
      "step: 16999, loss: 0.3200206756591797\n",
      "Checkpoint is saved\n",
      "step: 17049, loss: 0.4239502549171448\n",
      "step: 17099, loss: 0.4080490469932556\n",
      "step: 17149, loss: 0.344387412071228\n",
      "step: 17199, loss: 0.5042121410369873\n",
      "step: 17249, loss: 0.4739316403865814\n",
      "step: 17299, loss: 0.6729623079299927\n",
      "step: 17349, loss: 0.4114408493041992\n",
      "step: 17399, loss: 0.48660144209861755\n",
      "step: 17449, loss: 0.43918758630752563\n",
      "step: 17499, loss: 0.3731597363948822\n",
      "step: 17549, loss: 0.33028632402420044\n",
      "step: 17599, loss: 0.38207173347473145\n",
      "step: 17649, loss: 0.40243369340896606\n",
      "step: 17699, loss: 0.46617022156715393\n",
      "step: 17749, loss: 0.5282264947891235\n",
      "step: 17799, loss: 0.30075693130493164\n",
      "step: 17849, loss: 0.36453336477279663\n",
      "step: 17899, loss: 0.4478652775287628\n",
      "step: 17949, loss: 0.39650991559028625\n",
      "step: 17999, loss: 0.37091079354286194\n",
      "Checkpoint is saved\n",
      "step: 18049, loss: 0.2939032018184662\n",
      "step: 18099, loss: 0.3691931366920471\n",
      "step: 18149, loss: 0.3961634933948517\n",
      "step: 18199, loss: 0.479086697101593\n",
      "step: 18249, loss: 0.4574907422065735\n",
      "step: 18299, loss: 0.42224180698394775\n",
      "step: 18349, loss: 0.499921590089798\n",
      "step: 18399, loss: 0.3283875584602356\n",
      "step: 18449, loss: 0.34308958053588867\n",
      "step: 18499, loss: 0.32382553815841675\n",
      "step: 18549, loss: 0.5046370029449463\n",
      "step: 18599, loss: 0.5132087469100952\n",
      "step: 18649, loss: 0.4023488163948059\n",
      "step: 18699, loss: 0.35111451148986816\n",
      "step: 18749, loss: 0.3321755528450012\n",
      "step: 18799, loss: 0.3635385036468506\n",
      "step: 18849, loss: 0.42660221457481384\n",
      "step: 18899, loss: 0.402469664812088\n",
      "step: 18949, loss: 0.3446415364742279\n",
      "step: 18999, loss: 0.42691266536712646\n",
      "Checkpoint is saved\n",
      "step: 19049, loss: 0.259476900100708\n",
      "step: 19099, loss: 0.28442713618278503\n",
      "step: 19149, loss: 0.3337249159812927\n",
      "step: 19199, loss: 0.3187118172645569\n",
      "step: 19249, loss: 0.37281015515327454\n",
      "step: 19299, loss: 0.28354933857917786\n",
      "step: 19349, loss: 0.34416520595550537\n",
      "step: 19399, loss: 0.26689469814300537\n",
      "step: 19449, loss: 0.2708561420440674\n",
      "step: 19499, loss: 0.43827271461486816\n",
      "step: 19549, loss: 0.3395922780036926\n",
      "step: 19599, loss: 0.3187491297721863\n",
      "step: 19649, loss: 0.4876795709133148\n",
      "step: 19699, loss: 0.46433180570602417\n",
      "step: 19749, loss: 0.5371530652046204\n",
      "step: 19799, loss: 0.350195974111557\n",
      "step: 19849, loss: 0.44257983565330505\n",
      "step: 19899, loss: 0.31693461537361145\n",
      "step: 19949, loss: 0.31922754645347595\n",
      "step: 19999, loss: 0.34955018758773804\n",
      "Checkpoint is saved\n",
      "step: 20049, loss: 0.3805258274078369\n",
      "step: 20099, loss: 0.2927529811859131\n",
      "step: 20149, loss: 0.382702112197876\n",
      "step: 20199, loss: 0.5203654766082764\n",
      "step: 20249, loss: 0.36522555351257324\n",
      "step: 20299, loss: 0.3127305507659912\n",
      "step: 20349, loss: 0.43215906620025635\n",
      "step: 20399, loss: 0.28583186864852905\n",
      "step: 20449, loss: 0.40123865008354187\n",
      "step: 20499, loss: 0.30565664172172546\n",
      "step: 20549, loss: 0.2923470735549927\n",
      "step: 20599, loss: 0.3843867778778076\n",
      "step: 20649, loss: 0.3204842209815979\n",
      "step: 20699, loss: 0.29719263315200806\n",
      "step: 20749, loss: 0.27042055130004883\n",
      "step: 20799, loss: 0.2287338674068451\n",
      "step: 20849, loss: 0.3392220139503479\n",
      "step: 20899, loss: 0.33486396074295044\n",
      "step: 20949, loss: 0.3586077392101288\n",
      "step: 20999, loss: 0.29121559858322144\n",
      "Checkpoint is saved\n",
      "step: 21049, loss: 0.48699235916137695\n",
      "step: 21099, loss: 0.31730392575263977\n",
      "step: 21149, loss: 0.2693205177783966\n",
      "step: 21199, loss: 0.308191180229187\n",
      "step: 21249, loss: 0.2582847774028778\n",
      "step: 21299, loss: 0.2772837281227112\n",
      "step: 21349, loss: 0.33566713333129883\n",
      "step: 21399, loss: 0.32770776748657227\n",
      "step: 21449, loss: 0.40102607011795044\n",
      "step: 21499, loss: 0.34865885972976685\n",
      "step: 21549, loss: 0.3601566553115845\n",
      "step: 21599, loss: 0.4847952425479889\n",
      "step: 21649, loss: 0.38766902685165405\n",
      "step: 21699, loss: 0.37585777044296265\n",
      "step: 21749, loss: 0.3358265459537506\n",
      "step: 21799, loss: 0.4615229070186615\n",
      "step: 21849, loss: 0.33048903942108154\n",
      "step: 21899, loss: 0.3774825930595398\n",
      "step: 21949, loss: 0.36866527795791626\n",
      "step: 21999, loss: 0.3028727173805237\n",
      "Checkpoint is saved\n",
      "step: 22049, loss: 0.5498183369636536\n",
      "step: 22099, loss: 0.33852633833885193\n",
      "step: 22149, loss: 0.3213309943675995\n",
      "step: 22199, loss: 0.20517267286777496\n",
      "step: 22249, loss: 0.6050820350646973\n",
      "step: 22299, loss: 0.18896397948265076\n",
      "step: 22349, loss: 0.3009040653705597\n",
      "step: 22399, loss: 0.4573831260204315\n",
      "step: 22449, loss: 0.379294216632843\n",
      "step: 22499, loss: 0.3248010575771332\n",
      "step: 22549, loss: 0.4070962071418762\n",
      "step: 22599, loss: 0.27701860666275024\n",
      "step: 22649, loss: 0.4074890613555908\n",
      "step: 22699, loss: 0.34556496143341064\n",
      "step: 22749, loss: 0.3078605532646179\n",
      "step: 22799, loss: 0.2833021879196167\n",
      "step: 22849, loss: 0.2440788447856903\n",
      "step: 22899, loss: 0.45782965421676636\n",
      "step: 22949, loss: 0.41213691234588623\n",
      "step: 22999, loss: 0.26633453369140625\n",
      "Checkpoint is saved\n",
      "step: 23049, loss: 0.2913503646850586\n",
      "step: 23099, loss: 0.2940969467163086\n",
      "step: 23149, loss: 0.29861846566200256\n",
      "step: 23199, loss: 0.4033295512199402\n",
      "step: 23249, loss: 0.3755350708961487\n",
      "step: 23299, loss: 0.195476233959198\n",
      "step: 23349, loss: 0.3634101152420044\n",
      "step: 23399, loss: 0.3007793128490448\n",
      "step: 23449, loss: 0.23798851668834686\n",
      "step: 23499, loss: 0.30526310205459595\n",
      "step: 23549, loss: 0.266872376203537\n",
      "step: 23599, loss: 0.3120157718658447\n",
      "step: 23649, loss: 0.2786111831665039\n",
      "step: 23699, loss: 0.2656228542327881\n",
      "step: 23749, loss: 0.40893319249153137\n",
      "step: 23799, loss: 0.40489763021469116\n",
      "step: 23849, loss: 0.26233696937561035\n",
      "step: 23899, loss: 0.26796597242355347\n",
      "step: 23949, loss: 0.16399464011192322\n",
      "step: 23999, loss: 0.3422561287879944\n",
      "Checkpoint is saved\n",
      "step: 24049, loss: 0.24390681087970734\n",
      "step: 24099, loss: 0.3977457582950592\n",
      "step: 24149, loss: 0.26428377628326416\n",
      "step: 24199, loss: 0.2771349549293518\n",
      "step: 24249, loss: 0.28147727251052856\n",
      "step: 24299, loss: 0.18644583225250244\n",
      "step: 24349, loss: 0.28318092226982117\n",
      "step: 24399, loss: 0.3689814805984497\n",
      "step: 24449, loss: 0.24100908637046814\n",
      "step: 24499, loss: 0.36329784989356995\n",
      "step: 24549, loss: 0.28360503911972046\n",
      "step: 24599, loss: 0.28092512488365173\n",
      "step: 24649, loss: 0.26821663975715637\n",
      "step: 24699, loss: 0.2831619679927826\n",
      "step: 24749, loss: 0.24069926142692566\n",
      "step: 24799, loss: 0.32563909888267517\n",
      "step: 24849, loss: 0.262021541595459\n",
      "step: 24899, loss: 0.3991648554801941\n",
      "step: 24949, loss: 0.2971797585487366\n",
      "step: 24999, loss: 0.27673110365867615\n",
      "Checkpoint is saved\n",
      "step: 25049, loss: 0.30972743034362793\n",
      "step: 25099, loss: 0.3519851565361023\n",
      "step: 25149, loss: 0.31148383021354675\n",
      "step: 25199, loss: 0.3178771138191223\n",
      "step: 25249, loss: 0.32572418451309204\n",
      "step: 25299, loss: 0.2311917543411255\n",
      "step: 25349, loss: 0.19144207239151\n",
      "step: 25399, loss: 0.25336548686027527\n",
      "step: 25449, loss: 0.2885591685771942\n",
      "step: 25499, loss: 0.2888035178184509\n",
      "step: 25549, loss: 0.2332255244255066\n",
      "step: 25599, loss: 0.2557855248451233\n",
      "step: 25649, loss: 0.2775236964225769\n",
      "step: 25699, loss: 0.10483842343091965\n",
      "step: 25749, loss: 0.272274374961853\n",
      "step: 25799, loss: 0.3032539188861847\n",
      "step: 25849, loss: 0.2586234211921692\n",
      "step: 25899, loss: 0.2557826340198517\n",
      "step: 25949, loss: 0.2783259451389313\n",
      "step: 25999, loss: 0.1773061901330948\n",
      "Checkpoint is saved\n",
      "step: 26049, loss: 0.259246826171875\n",
      "step: 26099, loss: 0.34332960844039917\n",
      "step: 26149, loss: 0.26630985736846924\n",
      "step: 26199, loss: 0.2512492537498474\n",
      "step: 26249, loss: 0.29134565591812134\n",
      "step: 26299, loss: 0.17928728461265564\n",
      "step: 26349, loss: 0.2682156562805176\n",
      "step: 26399, loss: 0.27291250228881836\n",
      "step: 26449, loss: 0.31973254680633545\n",
      "step: 26499, loss: 0.25907549262046814\n",
      "step: 26549, loss: 0.29499518871307373\n",
      "step: 26599, loss: 0.197464719414711\n",
      "step: 26649, loss: 0.30742037296295166\n",
      "step: 26699, loss: 0.19424989819526672\n",
      "step: 26749, loss: 0.23564040660858154\n",
      "step: 26799, loss: 0.24854682385921478\n",
      "step: 26849, loss: 0.193395733833313\n",
      "step: 26899, loss: 0.24629494547843933\n",
      "step: 26949, loss: 0.18963587284088135\n",
      "step: 26999, loss: 0.2094276249408722\n",
      "Checkpoint is saved\n",
      "step: 27049, loss: 0.27131474018096924\n",
      "step: 27099, loss: 0.25282829999923706\n",
      "step: 27149, loss: 0.32394662499427795\n",
      "step: 27199, loss: 0.2552507221698761\n",
      "step: 27249, loss: 0.26683324575424194\n",
      "step: 27299, loss: 0.2521773874759674\n",
      "step: 27349, loss: 0.24302718043327332\n",
      "step: 27399, loss: 0.2838795781135559\n",
      "step: 27449, loss: 0.297730952501297\n",
      "step: 27499, loss: 0.3335196375846863\n",
      "step: 27549, loss: 0.3022146224975586\n",
      "step: 27599, loss: 0.1899055391550064\n",
      "step: 27649, loss: 0.25235313177108765\n",
      "step: 27699, loss: 0.2216387689113617\n",
      "step: 27749, loss: 0.17198997735977173\n",
      "step: 27799, loss: 0.20331686735153198\n",
      "step: 27849, loss: 0.24264973402023315\n",
      "step: 27899, loss: 0.238196462392807\n",
      "step: 27949, loss: 0.16895648837089539\n",
      "step: 27999, loss: 0.2701823115348816\n",
      "Checkpoint is saved\n",
      "step: 28049, loss: 0.17356139421463013\n",
      "step: 28099, loss: 0.28338974714279175\n",
      "step: 28149, loss: 0.26319506764411926\n",
      "step: 28199, loss: 0.2818495035171509\n",
      "step: 28249, loss: 0.314291387796402\n",
      "step: 28299, loss: 0.31188780069351196\n",
      "step: 28349, loss: 0.24290713667869568\n",
      "step: 28399, loss: 0.30769556760787964\n",
      "step: 28449, loss: 0.33247631788253784\n",
      "step: 28499, loss: 0.2456773817539215\n",
      "step: 28549, loss: 0.34596434235572815\n",
      "step: 28599, loss: 0.4156382381916046\n",
      "step: 28649, loss: 0.21475815773010254\n",
      "step: 28699, loss: 0.23111969232559204\n",
      "step: 28749, loss: 0.20443403720855713\n",
      "step: 28799, loss: 0.30867981910705566\n",
      "step: 28849, loss: 0.25652074813842773\n",
      "step: 28899, loss: 0.2738924026489258\n",
      "step: 28949, loss: 0.27951765060424805\n",
      "step: 28999, loss: 0.24814990162849426\n",
      "Checkpoint is saved\n",
      "step: 29049, loss: 0.24302555620670319\n",
      "step: 29099, loss: 0.32328376173973083\n",
      "step: 29149, loss: 0.2532663941383362\n",
      "step: 29199, loss: 0.19161522388458252\n",
      "step: 29249, loss: 0.235664963722229\n",
      "step: 29299, loss: 0.25429481267929077\n",
      "step: 29349, loss: 0.27416855096817017\n",
      "step: 29399, loss: 0.33092474937438965\n",
      "step: 29449, loss: 0.20302703976631165\n",
      "step: 29499, loss: 0.27273887395858765\n",
      "step: 29549, loss: 0.285514771938324\n",
      "step: 29599, loss: 0.23311005532741547\n",
      "step: 29649, loss: 0.28755518794059753\n",
      "step: 29699, loss: 0.21250417828559875\n",
      "step: 29749, loss: 0.21275734901428223\n",
      "step: 29799, loss: 0.35761725902557373\n",
      "step: 29849, loss: 0.25241532921791077\n",
      "step: 29899, loss: 0.3045899271965027\n",
      "step: 29949, loss: 0.18315789103507996\n",
      "step: 29999, loss: 0.17405322194099426\n",
      "Checkpoint is saved\n",
      "Training time for 30000 steps: 3411.540119409561s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 50 == 49 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 1000 == 999:\n",
    "            saver.save(sess, 'D:/NLP Project/Hindi English/checkpoints/', global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAGTCAYAAAAfltc1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEG70/xoVAY1S0fFUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a74ab1eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [hi_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [hi_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = hi_vocab_size,\n",
    "                                                embedding_size = 80,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    # let's translate these sentences     \n",
    "#     le_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "#                     'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    \n",
    "#     #le_sentences = en_vocab[:8]\n",
    "#     #le_sentences = [\"This\", \"time\", \"what\", \"why\", \"how\"]\n",
    "#     le_sentences = [\"I\", \"Absolute position\", \"Here she wrote a rectangle\", \"I should have liked to begin this story in the fashion of the fairy-tales .\"]\n",
    "#     en_sentences = []\n",
    "#     for each_sentence in le_sentences:\n",
    "#         en_sentences.append(each_sentence.lower())\n",
    "    \n",
    "#     #en_sentences = en_vocab[:20]\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "#     # padding to fit encoder input\n",
    "#     for i in range(len(en_sentences_encoded)):\n",
    "#         en_sentences_encoded[i] += (20 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "\n",
    "    for idx in range(1, 11):\n",
    "    \n",
    "        en_sentences_encoded = X_test[(idx-1)*1000:(idx)*1000]\n",
    "        hi_sentences_encoded = Y_test[(idx-1)*1000:(idx)*1000]\n",
    "        en_sentences = []\n",
    "        hi_sentences = []\n",
    "\n",
    "        import codecs\n",
    "        fp = codecs.open(\"D:/NLP Project/Hindi English/test_results\"+str(idx)+\".txt\", encoding = \"utf-8\", mode = \"w\")\n",
    "\n",
    "        for j in range(len(en_sentences_encoded)):\n",
    "            temp = \"\"\n",
    "            for i in range(len(en_sentences_encoded[j])):\n",
    "                temp += en_idx2word[en_sentences_encoded[j][i]] + \" \"\n",
    "            en_sentences.append(temp) \n",
    "\n",
    "        for j in range(len(hi_sentences_encoded)):\n",
    "            temp = \"\"\n",
    "            for i in range(len(hi_sentences_encoded[j])):\n",
    "                temp += hi_idx2word[hi_sentences_encoded[j][i]] + \" \"\n",
    "            hi_sentences.append(temp)\n",
    "\n",
    "        # restore all variables - use the last checkpoint saved\n",
    "        saver = tf.train.Saver()\n",
    "        path = tf.train.latest_checkpoint('D:/NLP Project/Hindi English/checkpoints/')\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # restore\n",
    "            saver.restore(sess, path)\n",
    "\n",
    "            # feed data into placeholders\n",
    "            feed = {}\n",
    "            for i in range(input_seq_len):\n",
    "                feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))])\n",
    "\n",
    "            feed[decoder_inputs[0].name] = np.array([hi_word2idx['<go>']] * len(en_sentences_encoded))\n",
    "\n",
    "            # translate\n",
    "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "\n",
    "            # decode seq.\n",
    "            for i in range(len(en_sentences_encoded)):\n",
    "                fp.write('\\n')\n",
    "                fp.write('{}.\\n--------------------------------'.format(i+1))\n",
    "                fp.write('\\n')\n",
    "                #print('{}.\\n--------------------------------'.format(i+1))\n",
    "                ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                #decode output sequence\n",
    "                words = decode_output(ouput_seq)\n",
    "\n",
    "                #print(en_sentences[i])\n",
    "                #print('\\n')\n",
    "                fp.write('Input\\t\\t - ')\n",
    "                actual_sentences = en_sentences[i].split()\n",
    "                for j in range(len(actual_sentences)):\n",
    "                    if actual_sentences[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        fp.write(actual_sentences[j]+\" \")\n",
    "                fp.write('\\n')\n",
    "\n",
    "                fp.write('Actual\\t\\t - ')\n",
    "                actual_output = hi_sentences[i].split()\n",
    "                for j in range(len(actual_output)):\n",
    "                    if actual_output[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        fp.write(actual_output[j] + \" \")\n",
    "                fp.write('\\n')\n",
    "\n",
    "                fp.write('Predicted\\t\\t - ')\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        #print(words[i], end=' ')\n",
    "                        fp.write(words[i]+\" \")\n",
    "\n",
    "                #print('\\n--------------------------------')\n",
    "                fp.write('\\n--------------------------------')\n",
    "                fp.write('\\n')\n",
    "        fp.close()\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "mobile bookmarks \n",
      "\n",
      "Actual\t\t - \n",
      "मोबाइल बुकमार्क \n",
      "\n",
      "Predicted\t\t - \n",
      "मोबाइल बुकमार्क \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "at certain places original sanskrit text are <ukn> available \n",
      "\n",
      "Actual\t\t - \n",
      "कहीं-कहीं मूल संस्कृत पाठ भी उपलब्ध है। \n",
      "\n",
      "Predicted\t\t - \n",
      "कुछ <ukn> <ukn> पाठ <ukn> उपलब्ध हैं  पर उपलब्ध \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "number of <ukn> \n",
      "\n",
      "Actual\t\t - \n",
      "<ukn> की संख्या \n",
      "\n",
      "Predicted\t\t - \n",
      "<ukn> की संख्या \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "which side of the notebook holds the tabs \n",
      "\n",
      "Actual\t\t - \n",
      "नोटबुक के किस तरफ टैब रखना है \n",
      "\n",
      "Predicted\t\t - \n",
      "नोटबुक के प्रति टैब टैब रखना है \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "children became more competent \n",
      "\n",
      "Actual\t\t - \n",
      "बच्चे ज्यादा निपुण हुए \n",
      "\n",
      "Predicted\t\t - \n",
      "बच्चे बन कर डा \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "the young one <ukn> with its mouth \n",
      "\n",
      "Actual\t\t - \n",
      "नवजात बच्चा मुख से स्तन चूसता है \n",
      "\n",
      "Predicted\t\t - \n",
      "अपने पास अपने पास अपने पास   \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "consumed products \n",
      "\n",
      "Actual\t\t - \n",
      "खपत उत्पाद \n",
      "\n",
      "Predicted\t\t - \n",
      "खपत उत्पादों \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "open cluster \n",
      "\n",
      "Actual\t\t - \n",
      "खुला तारागुच्छ \n",
      "\n",
      "Predicted\t\t - \n",
      "खुला तारागुच्छ \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "could not update folder permissions \n",
      "\n",
      "Actual\t\t - \n",
      "फ़ोल्डर अनुमतियां अद्यतन नहीं कर सका \n",
      "\n",
      "Predicted\t\t - \n",
      "फ़ोल्डर को फ़ोल्डर अद्यतन नहीं कर सका \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Input\t\t - \n",
      "at present it consists of isolated mountains between which the ocean flows \n",
      "\n",
      "Actual\t\t - \n",
      "इस समय इसमें <ukn> हुए पर्वत हैं ऋनके बीच से समुद्र बहता है \n",
      "\n",
      "Predicted\t\t - \n",
      "इसके ऊपर  बाहर से <ukn> तक  समुद्र से <ukn> होते हैं    \n",
      "--------------------------------\n",
      "\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [hi_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [hi_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = hi_vocab_size,\n",
    "                                                embedding_size = 80,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    # let's translate these sentences     \n",
    "#     le_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "#                     'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    \n",
    "#     #le_sentences = en_vocab[:8]\n",
    "#     #le_sentences = [\"This\", \"time\", \"what\", \"why\", \"how\"]\n",
    "#     le_sentences = [\"I\", \"Absolute position\", \"Here she wrote a rectangle\", \"I should have liked to begin this story in the fashion of the fairy-tales .\"]\n",
    "#     en_sentences = []\n",
    "#     for each_sentence in le_sentences:\n",
    "#         en_sentences.append(each_sentence.lower())\n",
    "    \n",
    "#     #en_sentences = en_vocab[:20]\n",
    "#     en_sentences_encoded = [[en_word2idx.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    \n",
    "#     # padding to fit encoder input\n",
    "#     for i in range(len(en_sentences_encoded)):\n",
    "#         en_sentences_encoded[i] += (20 - len(en_sentences_encoded[i])) * [en_word2idx['<pad>']]\n",
    "    \n",
    "    en_sentences_encoded = X_test[50:60]\n",
    "    hi_sentences_encoded = Y_test[50:60]\n",
    "    en_sentences = []\n",
    "    hi_sentences = []\n",
    "\n",
    "    for j in range(len(en_sentences_encoded)):\n",
    "        temp = \"\"\n",
    "        for i in range(len(en_sentences_encoded[j])):\n",
    "            temp += en_idx2word[en_sentences_encoded[j][i]] + \" \"\n",
    "        en_sentences.append(temp) \n",
    "\n",
    "    for j in range(len(hi_sentences_encoded)):\n",
    "        temp = \"\"\n",
    "        for i in range(len(hi_sentences_encoded[j])):\n",
    "            temp += hi_idx2word[hi_sentences_encoded[j][i]] + \" \"\n",
    "        hi_sentences.append(temp)\n",
    "\n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('D:/NLP Project/Hindi English/checkpoints/')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "\n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))])\n",
    "\n",
    "        feed[decoder_inputs[0].name] = np.array([hi_word2idx['<go>']] * len(en_sentences_encoded))\n",
    "\n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "\n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print('\\n')\n",
    "            print('{}.\\n--------------------------------'.format(i+1))\n",
    "            print('\\n')\n",
    "            #print('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "\n",
    "            #print(en_sentences[i])\n",
    "            #print('\\n')\n",
    "            print('Input\\t\\t - ')\n",
    "            actual_sentences = en_sentences[i].split()\n",
    "            for j in range(len(actual_sentences)):\n",
    "                if actual_sentences[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print(actual_sentences[j], end=' ')\n",
    "            print('\\n')\n",
    "\n",
    "            print('Actual\\t\\t - ')\n",
    "            actual_output = hi_sentences[i].split()\n",
    "            for j in range(len(actual_output)):\n",
    "                if actual_output[j] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print(actual_output[j], end = ' ')\n",
    "            print('\\n')\n",
    "\n",
    "            print('Predicted\\t\\t - ')\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    #print(words[i], end=' ')\n",
    "                    print(words[i], end = ' ')\n",
    "\n",
    "            #print('\\n--------------------------------')\n",
    "            print('\\n--------------------------------')\n",
    "            print('\\n')\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for', 'it', 'this', 'on', 'you', 'was', 'as', 'are', 'with', 'be', 'not']\n",
      "True\n",
      "14749\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:20])\n",
    "print(\"इस\" in hi_vocab)\n",
    "print(len(X_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

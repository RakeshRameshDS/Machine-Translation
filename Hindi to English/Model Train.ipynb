{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"./data.p\"\n",
    "X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['के', '', 'में', 'है', 'की', 'और', 'से', 'को', 'का', 'हैं', 'कि', 'पर', 'एक', 'नहीं', 'लिए', 'यह', 'भी', 'इस', 'कर', 'ने', 'हो', '।', 'ही', 'करने', 'जो', 'तो', 'किया', 'या', 'था', 'आप']\n",
      "['the', '', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for', 'it', 'this', 'on', 'you', 'was', 'as', 'are', 'with', 'be', 'not', 'by', 'or', 'he', 'from', 'his', 'have', 'but', 'at', 'an', 'which']\n"
     ]
    }
   ],
   "source": [
    "print(l1_vocab[:30])\n",
    "print(l2_vocab[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 20):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [l1_word2idx['<pad>']]\n",
    "        y[i] = [l2_word2idx['<go>']] + y[i] + [l2_word2idx['<eos>']] + (length-len(y[i])) * [l2_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "l1_vocab_size = len(l1_vocab) + 2 # + <pad>, <ukn>\n",
    "l2_vocab_size = len(l2_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{0}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [l2_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [l2_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = l1_vocab_size,\n",
    "                                            num_decoder_symbols = l2_vocab_size,\n",
    "                                            embedding_size = 80,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(logits, labels):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = l2_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.nn.seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = l2_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == l2_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(l2_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 3e-3\n",
    "batch_size = 8\n",
    "steps = 40000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.99).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 9.30533218383789\n",
      "step: 49, loss: 6.387561798095703\n",
      "step: 99, loss: 6.536971092224121\n",
      "step: 149, loss: 5.150188446044922\n",
      "step: 199, loss: 4.962061882019043\n",
      "step: 249, loss: 5.49918270111084\n",
      "step: 299, loss: 5.422075271606445\n",
      "step: 349, loss: 5.663182258605957\n",
      "step: 399, loss: 4.708776950836182\n",
      "step: 449, loss: 4.79289436340332\n",
      "step: 499, loss: 4.60256814956665\n",
      "step: 549, loss: 4.5296502113342285\n",
      "step: 599, loss: 4.785130500793457\n",
      "step: 649, loss: 4.505695819854736\n",
      "step: 699, loss: 4.381748199462891\n",
      "step: 749, loss: 4.189996719360352\n",
      "step: 799, loss: 4.391980171203613\n",
      "step: 849, loss: 3.696199893951416\n",
      "step: 899, loss: 3.9169869422912598\n",
      "step: 949, loss: 3.935019016265869\n",
      "step: 999, loss: 3.7516911029815674\n",
      "Checkpoint is saved\n",
      "step: 1049, loss: 3.5869140625\n",
      "step: 1099, loss: 3.64778470993042\n",
      "step: 1149, loss: 3.3666839599609375\n",
      "step: 1199, loss: 3.590575695037842\n",
      "step: 1249, loss: 3.3249871730804443\n",
      "step: 1299, loss: 3.3963112831115723\n",
      "step: 1349, loss: 3.075110912322998\n",
      "step: 1399, loss: 3.031625509262085\n",
      "step: 1449, loss: 3.0112500190734863\n",
      "step: 1499, loss: 2.5854063034057617\n",
      "step: 1549, loss: 3.2372422218322754\n",
      "step: 1599, loss: 2.9265801906585693\n",
      "step: 1649, loss: 2.9837536811828613\n",
      "step: 1699, loss: 2.6239938735961914\n",
      "step: 1749, loss: 3.7849936485290527\n",
      "step: 1799, loss: 3.170957565307617\n",
      "step: 1849, loss: 3.1223161220550537\n",
      "step: 1899, loss: 2.55281925201416\n",
      "step: 1949, loss: 2.8614959716796875\n",
      "step: 1999, loss: 2.6823747158050537\n",
      "Checkpoint is saved\n",
      "step: 2049, loss: 2.495699644088745\n",
      "step: 2099, loss: 2.314389228820801\n",
      "step: 2149, loss: 2.760519504547119\n",
      "step: 2199, loss: 2.5892934799194336\n",
      "step: 2249, loss: 2.450866222381592\n",
      "step: 2299, loss: 6.06942081451416\n",
      "step: 2349, loss: 2.379366397857666\n",
      "step: 2399, loss: 2.4924564361572266\n",
      "step: 2449, loss: 2.3201818466186523\n",
      "step: 2499, loss: 2.2686619758605957\n",
      "step: 2549, loss: 1.9881349802017212\n",
      "step: 2599, loss: 2.2477269172668457\n",
      "step: 2649, loss: 2.2666475772857666\n",
      "step: 2699, loss: 2.261876344680786\n",
      "step: 2749, loss: 1.8488833904266357\n",
      "step: 2799, loss: 2.276667833328247\n",
      "step: 2849, loss: 2.042041778564453\n",
      "step: 2899, loss: 1.6293580532073975\n",
      "step: 2949, loss: 2.067354202270508\n",
      "step: 2999, loss: 2.103484869003296\n",
      "Checkpoint is saved\n",
      "step: 3049, loss: 1.8655922412872314\n",
      "step: 3099, loss: 1.9648268222808838\n",
      "step: 3149, loss: 1.6547943353652954\n",
      "step: 3199, loss: 1.9130712747573853\n",
      "step: 3249, loss: 1.970324993133545\n",
      "step: 3299, loss: 1.774843454360962\n",
      "step: 3349, loss: 1.9023408889770508\n",
      "step: 3399, loss: 1.9168853759765625\n",
      "step: 3449, loss: 1.9863585233688354\n",
      "step: 3499, loss: 1.9646952152252197\n",
      "step: 3549, loss: 1.7064869403839111\n",
      "step: 3599, loss: 1.7024967670440674\n",
      "step: 3649, loss: 1.770540714263916\n",
      "step: 3699, loss: 2.084317207336426\n",
      "step: 3749, loss: 2.0873465538024902\n",
      "step: 3799, loss: 1.7602131366729736\n",
      "step: 3849, loss: 1.8418269157409668\n",
      "step: 3899, loss: 1.2573202848434448\n",
      "step: 3949, loss: 1.7467107772827148\n",
      "step: 3999, loss: 1.7205113172531128\n",
      "Checkpoint is saved\n",
      "step: 4049, loss: 1.8509557247161865\n",
      "step: 4099, loss: 1.7210056781768799\n",
      "step: 4149, loss: 1.960784673690796\n",
      "step: 4199, loss: 1.7822508811950684\n",
      "step: 4249, loss: 1.7196567058563232\n",
      "step: 4299, loss: 1.6960313320159912\n",
      "step: 4349, loss: 1.5736619234085083\n",
      "step: 4399, loss: 1.6318389177322388\n",
      "step: 4449, loss: 1.2251567840576172\n",
      "step: 4499, loss: 1.311048984527588\n",
      "step: 4549, loss: 1.4711596965789795\n",
      "step: 4599, loss: 1.8222861289978027\n",
      "step: 4649, loss: 1.1863000392913818\n",
      "step: 4699, loss: 1.8565454483032227\n",
      "step: 4749, loss: 1.66419517993927\n",
      "step: 4799, loss: 1.4481918811798096\n",
      "step: 4849, loss: 1.4320166110992432\n",
      "step: 4899, loss: 1.328385829925537\n",
      "step: 4949, loss: 1.2732021808624268\n",
      "step: 4999, loss: 1.5235342979431152\n",
      "Checkpoint is saved\n",
      "step: 5049, loss: 1.3544007539749146\n",
      "step: 5099, loss: 1.4873738288879395\n",
      "step: 5149, loss: 1.3983622789382935\n",
      "step: 5199, loss: 1.3223823308944702\n",
      "step: 5249, loss: 1.1010018587112427\n",
      "step: 5299, loss: 1.6018619537353516\n",
      "step: 5349, loss: 1.2161836624145508\n",
      "step: 5399, loss: 0.9880514740943909\n",
      "step: 5449, loss: 1.1955523490905762\n",
      "step: 5499, loss: 1.3880969285964966\n",
      "step: 5549, loss: 1.336548924446106\n",
      "step: 5599, loss: 1.2376487255096436\n",
      "step: 5649, loss: 1.3016941547393799\n",
      "step: 5699, loss: 1.2547175884246826\n",
      "step: 5749, loss: 1.578656554222107\n",
      "step: 5799, loss: 1.071647047996521\n",
      "step: 5849, loss: 1.2100855112075806\n",
      "step: 5899, loss: 1.3465359210968018\n",
      "step: 5949, loss: 0.912312388420105\n",
      "step: 5999, loss: 1.3341047763824463\n",
      "Checkpoint is saved\n",
      "step: 6049, loss: 1.296116828918457\n",
      "step: 6099, loss: 1.4807841777801514\n",
      "step: 6149, loss: 1.341508150100708\n",
      "step: 6199, loss: 1.104665756225586\n",
      "step: 6249, loss: 1.3034749031066895\n",
      "step: 6299, loss: 1.3702208995819092\n",
      "step: 6349, loss: 1.1033581495285034\n",
      "step: 6399, loss: 1.074141263961792\n",
      "step: 6449, loss: 1.4497703313827515\n",
      "step: 6499, loss: 0.8291776180267334\n",
      "step: 6549, loss: 1.4760305881500244\n",
      "step: 6599, loss: 1.145472526550293\n",
      "step: 6649, loss: 1.153111457824707\n",
      "step: 6699, loss: 1.0811253786087036\n",
      "step: 6749, loss: 1.1668715476989746\n",
      "step: 6799, loss: 1.2767210006713867\n",
      "step: 6849, loss: 1.009342908859253\n",
      "step: 6899, loss: 1.1775257587432861\n",
      "step: 6949, loss: 1.0917631387710571\n",
      "step: 6999, loss: 1.0432417392730713\n",
      "Checkpoint is saved\n",
      "step: 7049, loss: 0.8451505899429321\n",
      "step: 7099, loss: 0.9608033299446106\n",
      "step: 7149, loss: 1.1372144222259521\n",
      "step: 7199, loss: 1.0504651069641113\n",
      "step: 7249, loss: 1.1173341274261475\n",
      "step: 7299, loss: 1.0981059074401855\n",
      "step: 7349, loss: 0.983608603477478\n",
      "step: 7399, loss: 1.0420784950256348\n",
      "step: 7449, loss: 1.2133123874664307\n",
      "step: 7499, loss: 0.850348949432373\n",
      "step: 7549, loss: 1.0289020538330078\n",
      "step: 7599, loss: 1.0602519512176514\n",
      "step: 7649, loss: 1.0103087425231934\n",
      "step: 7699, loss: 1.0126020908355713\n",
      "step: 7749, loss: 0.9694951772689819\n",
      "step: 7799, loss: 0.972878098487854\n",
      "step: 7849, loss: 0.9709440469741821\n",
      "step: 7899, loss: 0.9894224405288696\n",
      "step: 7949, loss: 0.8028010129928589\n",
      "step: 7999, loss: 1.0589098930358887\n",
      "Checkpoint is saved\n",
      "step: 8049, loss: 1.3621920347213745\n",
      "step: 8099, loss: 1.0025300979614258\n",
      "step: 8149, loss: 0.9586301445960999\n",
      "step: 8199, loss: 0.9272458553314209\n",
      "step: 8249, loss: 0.9988536238670349\n",
      "step: 8299, loss: 1.0820856094360352\n",
      "step: 8349, loss: 1.023303747177124\n",
      "step: 8399, loss: 0.9836173057556152\n",
      "step: 8449, loss: 0.7123317718505859\n",
      "step: 8499, loss: 1.1800918579101562\n",
      "step: 8549, loss: 0.9167690873146057\n",
      "step: 8599, loss: 0.9678353667259216\n",
      "step: 8649, loss: 0.9548209309577942\n",
      "step: 8699, loss: 1.0120818614959717\n",
      "step: 8749, loss: 1.0761547088623047\n",
      "step: 8799, loss: 0.9731208682060242\n",
      "step: 8849, loss: 0.87269127368927\n",
      "step: 8899, loss: 0.7946197390556335\n",
      "step: 8949, loss: 0.8914942741394043\n",
      "step: 8999, loss: 0.8899344205856323\n",
      "Checkpoint is saved\n",
      "step: 9049, loss: 0.9922196865081787\n",
      "step: 9099, loss: 1.1223678588867188\n",
      "step: 9149, loss: 0.7829602956771851\n",
      "step: 9199, loss: 0.8142873048782349\n",
      "step: 9249, loss: 0.9371658563613892\n",
      "step: 9299, loss: 0.8574869632720947\n",
      "step: 9349, loss: 1.0575578212738037\n",
      "step: 9399, loss: 0.8036292791366577\n",
      "step: 9449, loss: 0.9671921730041504\n",
      "step: 9499, loss: 0.8870162963867188\n",
      "step: 9549, loss: 0.745086133480072\n",
      "step: 9599, loss: 0.7013550400733948\n",
      "step: 9649, loss: 0.9875446557998657\n",
      "step: 9699, loss: 0.9005122780799866\n",
      "step: 9749, loss: 0.7581745386123657\n",
      "step: 9799, loss: 0.7584357857704163\n",
      "step: 9849, loss: 0.8260590434074402\n",
      "step: 9899, loss: 0.9154691100120544\n",
      "step: 9949, loss: 0.8134137392044067\n",
      "step: 9999, loss: 0.7523153424263\n",
      "Checkpoint is saved\n",
      "step: 10049, loss: 0.8329929113388062\n",
      "step: 10099, loss: 0.7553388476371765\n",
      "step: 10149, loss: 0.6869720816612244\n",
      "step: 10199, loss: 0.9336730241775513\n",
      "step: 10249, loss: 0.8548296689987183\n",
      "step: 10299, loss: 0.7086631059646606\n",
      "step: 10349, loss: 0.9884293079376221\n",
      "step: 10399, loss: 0.8197610378265381\n",
      "step: 10449, loss: 0.841812252998352\n",
      "step: 10499, loss: 0.9094300270080566\n",
      "step: 10549, loss: 0.6167044639587402\n",
      "step: 10599, loss: 0.6966829299926758\n",
      "step: 10649, loss: 0.724818229675293\n",
      "step: 10699, loss: 0.9202524423599243\n",
      "step: 10749, loss: 0.6524006128311157\n",
      "step: 10799, loss: 0.8304218053817749\n",
      "step: 10849, loss: 0.9090350270271301\n",
      "step: 10899, loss: 0.8378272652626038\n",
      "step: 10949, loss: 0.8151258230209351\n",
      "step: 10999, loss: 0.8464881181716919\n",
      "Checkpoint is saved\n",
      "step: 11049, loss: 0.7228810787200928\n",
      "step: 11099, loss: 0.6977038383483887\n",
      "step: 11149, loss: 0.7940570712089539\n",
      "step: 11199, loss: 0.8152010440826416\n",
      "step: 11249, loss: 0.647724986076355\n",
      "step: 11299, loss: 0.7102491855621338\n",
      "step: 11349, loss: 0.719707727432251\n",
      "step: 11399, loss: 0.7208731174468994\n",
      "step: 11449, loss: 0.6100749373435974\n",
      "step: 11499, loss: 1.1131850481033325\n",
      "step: 11549, loss: 0.6728265881538391\n",
      "step: 11599, loss: 0.8310849666595459\n",
      "step: 11649, loss: 0.6550308465957642\n",
      "step: 11699, loss: 0.8510875701904297\n",
      "step: 11749, loss: 0.47635617852211\n",
      "step: 11799, loss: 0.7340335249900818\n",
      "step: 11849, loss: 0.7728897333145142\n",
      "step: 11899, loss: 0.6330906748771667\n",
      "step: 11949, loss: 0.5994798541069031\n",
      "step: 11999, loss: 0.7001737356185913\n",
      "Checkpoint is saved\n",
      "step: 12049, loss: 0.6107848882675171\n",
      "step: 12099, loss: 0.522639274597168\n",
      "step: 12149, loss: 0.44063040614128113\n",
      "step: 12199, loss: 0.5799432396888733\n",
      "step: 12249, loss: 0.5961538553237915\n",
      "step: 12299, loss: 0.6457306146621704\n",
      "step: 12349, loss: 0.46219027042388916\n",
      "step: 12399, loss: 0.806826651096344\n",
      "step: 12449, loss: 0.6640947461128235\n",
      "step: 12499, loss: 0.6575509309768677\n",
      "step: 12549, loss: 0.850226640701294\n",
      "step: 12599, loss: 0.6246028542518616\n",
      "step: 12649, loss: 0.6281779408454895\n",
      "step: 12699, loss: 0.9030419588088989\n",
      "step: 12749, loss: 0.7287561297416687\n",
      "step: 12799, loss: 0.6884211301803589\n",
      "step: 12849, loss: 0.6553283929824829\n",
      "step: 12899, loss: 0.6560225486755371\n",
      "step: 12949, loss: 0.5338282585144043\n",
      "step: 12999, loss: 0.6172837018966675\n",
      "Checkpoint is saved\n",
      "step: 13049, loss: 0.6962792873382568\n",
      "step: 13099, loss: 0.593475341796875\n",
      "step: 13149, loss: 0.6758891344070435\n",
      "step: 13199, loss: 0.5912906527519226\n",
      "step: 13249, loss: 0.6602760553359985\n",
      "step: 13299, loss: 0.8697149753570557\n",
      "step: 13349, loss: 0.6715370416641235\n",
      "step: 13399, loss: 0.5196534991264343\n",
      "step: 13449, loss: 0.669493556022644\n",
      "step: 13499, loss: 0.6159205436706543\n",
      "step: 13549, loss: 0.6049530506134033\n",
      "step: 13599, loss: 0.7762312889099121\n",
      "step: 13649, loss: 0.7399179935455322\n",
      "step: 13699, loss: 0.6231777667999268\n",
      "step: 13749, loss: 0.6588693857192993\n",
      "step: 13799, loss: 0.6063822507858276\n",
      "step: 13849, loss: 0.5535315275192261\n",
      "step: 13899, loss: 0.7061328887939453\n",
      "step: 13949, loss: 0.580076277256012\n",
      "step: 13999, loss: 0.8560091853141785\n",
      "Checkpoint is saved\n",
      "step: 14049, loss: 0.5223763585090637\n",
      "step: 14099, loss: 0.5547432899475098\n",
      "step: 14149, loss: 0.6070767641067505\n",
      "step: 14199, loss: 0.5390249490737915\n",
      "step: 14249, loss: 0.5322990417480469\n",
      "step: 14299, loss: 0.6100279092788696\n",
      "step: 14349, loss: 0.41754549741744995\n",
      "step: 14399, loss: 0.6948645710945129\n",
      "step: 14449, loss: 0.5515949726104736\n",
      "step: 14499, loss: 0.6610079407691956\n",
      "step: 14549, loss: 0.6735105514526367\n",
      "step: 14599, loss: 0.5354387760162354\n",
      "step: 14649, loss: 0.5541051030158997\n",
      "step: 14699, loss: 0.4950464069843292\n",
      "step: 14749, loss: 0.6253140568733215\n",
      "step: 14799, loss: 0.5258269906044006\n",
      "step: 14849, loss: 0.5187339782714844\n",
      "step: 14899, loss: 0.5271075367927551\n",
      "step: 14949, loss: 0.6851354837417603\n",
      "step: 14999, loss: 0.42066848278045654\n",
      "Checkpoint is saved\n",
      "step: 15049, loss: 0.5587468147277832\n",
      "step: 15099, loss: 0.4659099876880646\n",
      "step: 15149, loss: 0.5141460299491882\n",
      "step: 15199, loss: 0.47517117857933044\n",
      "step: 15249, loss: 0.607204794883728\n",
      "step: 15299, loss: 0.49966177344322205\n",
      "step: 15349, loss: 0.5680943727493286\n",
      "step: 15399, loss: 0.8054409027099609\n",
      "step: 15449, loss: 0.5837153196334839\n",
      "step: 15499, loss: 0.5466687679290771\n",
      "step: 15549, loss: 0.46136051416397095\n",
      "step: 15599, loss: 0.51207435131073\n",
      "step: 15649, loss: 0.507200300693512\n",
      "step: 15699, loss: 0.5854182839393616\n",
      "step: 15749, loss: 0.4455264210700989\n",
      "step: 15799, loss: 0.5350394248962402\n",
      "step: 15849, loss: 0.4981399178504944\n",
      "step: 15899, loss: 0.5679000616073608\n",
      "step: 15949, loss: 0.5088651776313782\n",
      "step: 15999, loss: 0.4361943304538727\n",
      "Checkpoint is saved\n",
      "step: 16049, loss: 0.492057204246521\n",
      "step: 16099, loss: 0.8251564502716064\n",
      "step: 16149, loss: 0.4871026873588562\n",
      "step: 16199, loss: 0.5276780128479004\n",
      "step: 16249, loss: 0.680790364742279\n",
      "step: 16299, loss: 0.5325943827629089\n",
      "step: 16349, loss: 0.49849769473075867\n",
      "step: 16399, loss: 0.4429943263530731\n",
      "step: 16449, loss: 0.660287618637085\n",
      "step: 16499, loss: 0.3969085216522217\n",
      "step: 16549, loss: 0.4585392475128174\n",
      "step: 16599, loss: 0.5519552230834961\n",
      "step: 16649, loss: 0.5649368762969971\n",
      "step: 16699, loss: 0.46630412340164185\n",
      "step: 16749, loss: 0.5420345067977905\n",
      "step: 16799, loss: 0.661697506904602\n",
      "step: 16849, loss: 0.4950296878814697\n",
      "step: 16899, loss: 0.5607611536979675\n",
      "step: 16949, loss: 0.4635412395000458\n",
      "step: 16999, loss: 0.3848998248577118\n",
      "Checkpoint is saved\n",
      "step: 17049, loss: 0.45246192812919617\n",
      "step: 17099, loss: 0.6426502466201782\n",
      "step: 17149, loss: 0.3884003758430481\n",
      "step: 17199, loss: 0.5914071798324585\n",
      "step: 17249, loss: 0.6136021614074707\n",
      "step: 17299, loss: 0.5378578901290894\n",
      "step: 17349, loss: 0.5446211099624634\n",
      "step: 17399, loss: 0.5067974925041199\n",
      "step: 17449, loss: 0.556125819683075\n",
      "step: 17499, loss: 0.45868003368377686\n",
      "step: 17549, loss: 0.5362125039100647\n",
      "step: 17599, loss: 0.5224783420562744\n",
      "step: 17649, loss: 0.45045024156570435\n",
      "step: 17699, loss: 0.42657724022865295\n",
      "step: 17749, loss: 0.5257136821746826\n",
      "step: 17799, loss: 0.4919831454753876\n",
      "step: 17849, loss: 0.595018744468689\n",
      "step: 17899, loss: 0.5778444409370422\n",
      "step: 17949, loss: 0.4672216773033142\n",
      "step: 17999, loss: 0.35545963048934937\n",
      "Checkpoint is saved\n",
      "step: 18049, loss: 1.1309609413146973\n",
      "step: 18099, loss: 0.9074095487594604\n",
      "step: 18149, loss: 0.4649842083454132\n",
      "step: 18199, loss: 0.6297167539596558\n",
      "step: 18249, loss: 0.3970111012458801\n",
      "step: 18299, loss: 0.5511805415153503\n",
      "step: 18349, loss: 0.5707106590270996\n",
      "step: 18399, loss: 0.48223742842674255\n",
      "step: 18449, loss: 0.54393470287323\n",
      "step: 18499, loss: 0.4742918014526367\n",
      "step: 18549, loss: 0.35519957542419434\n",
      "step: 18599, loss: 0.462352991104126\n",
      "step: 18649, loss: 0.4917429983615875\n",
      "step: 18699, loss: 0.3047119081020355\n",
      "step: 18749, loss: 0.3194397985935211\n",
      "step: 18799, loss: 0.4688701033592224\n",
      "step: 18849, loss: 0.582811713218689\n",
      "step: 18899, loss: 0.3915190100669861\n",
      "step: 18949, loss: 0.3360588550567627\n",
      "step: 18999, loss: 0.6561168432235718\n",
      "Checkpoint is saved\n",
      "step: 19049, loss: 0.44720879197120667\n",
      "step: 19099, loss: 0.3607921600341797\n",
      "step: 19149, loss: 0.5021059513092041\n",
      "step: 19199, loss: 0.3765665292739868\n",
      "step: 19249, loss: 0.4474606513977051\n",
      "step: 19299, loss: 0.29327619075775146\n",
      "step: 19349, loss: 0.4322255551815033\n",
      "step: 19399, loss: 0.43578892946243286\n",
      "step: 19449, loss: 0.45886242389678955\n",
      "step: 19499, loss: 0.3936092257499695\n",
      "step: 19549, loss: 0.3859366178512573\n",
      "step: 19599, loss: 0.4671228528022766\n",
      "step: 19649, loss: 0.27376672625541687\n",
      "step: 19699, loss: 0.46313828229904175\n",
      "step: 19749, loss: 0.3411625027656555\n",
      "step: 19799, loss: 0.4211396872997284\n",
      "step: 19849, loss: 0.44554173946380615\n",
      "step: 19899, loss: 0.49300694465637207\n",
      "step: 19949, loss: 0.37079188227653503\n",
      "step: 19999, loss: 0.4708469808101654\n",
      "Checkpoint is saved\n",
      "step: 20049, loss: 0.32395845651626587\n",
      "step: 20099, loss: 0.5419027805328369\n",
      "step: 20149, loss: 0.31349897384643555\n",
      "step: 20199, loss: 0.34020400047302246\n",
      "step: 20249, loss: 0.40087491273880005\n",
      "step: 20299, loss: 0.38723722100257874\n",
      "step: 20349, loss: 0.5154474973678589\n",
      "step: 20399, loss: 0.42514440417289734\n",
      "step: 20449, loss: 0.31253373622894287\n",
      "step: 20499, loss: 0.39805349707603455\n",
      "step: 20549, loss: 0.6758722066879272\n",
      "step: 20599, loss: 0.4689779281616211\n",
      "step: 20649, loss: 0.45604825019836426\n",
      "step: 20699, loss: 0.30008357763290405\n",
      "step: 20749, loss: 0.5053826570510864\n",
      "step: 20799, loss: 0.36595597863197327\n",
      "step: 20849, loss: 0.3805071711540222\n",
      "step: 20899, loss: 0.4226985573768616\n",
      "step: 20949, loss: 0.34625494480133057\n",
      "step: 20999, loss: 0.40935438871383667\n",
      "Checkpoint is saved\n",
      "step: 21049, loss: 0.42263901233673096\n",
      "step: 21099, loss: 5.983868598937988\n",
      "step: 21149, loss: 0.35718482732772827\n",
      "step: 21199, loss: 0.4722508192062378\n",
      "step: 21249, loss: 0.41406190395355225\n",
      "step: 21299, loss: 0.38056832551956177\n",
      "step: 21349, loss: 0.408596009016037\n",
      "step: 21399, loss: 0.3885893225669861\n",
      "step: 21449, loss: 0.4008256196975708\n",
      "step: 21499, loss: 0.38428792357444763\n",
      "step: 21549, loss: 0.5362433195114136\n",
      "step: 21599, loss: 0.4551284909248352\n",
      "step: 21649, loss: 0.39967989921569824\n",
      "step: 21699, loss: 0.34168240427970886\n",
      "step: 21749, loss: 0.3700835406780243\n",
      "step: 21799, loss: 0.3883301913738251\n",
      "step: 21849, loss: 0.3664621412754059\n",
      "step: 21899, loss: 0.40643417835235596\n",
      "step: 21949, loss: 0.3412054181098938\n",
      "step: 21999, loss: 0.35887861251831055\n",
      "Checkpoint is saved\n",
      "step: 22049, loss: 0.3114158511161804\n",
      "step: 22099, loss: 0.43304508924484253\n",
      "step: 22149, loss: 0.43521130084991455\n",
      "step: 22199, loss: 0.4567325711250305\n",
      "step: 22249, loss: 0.25902509689331055\n",
      "step: 22299, loss: 0.48282450437545776\n",
      "step: 22349, loss: 0.39940595626831055\n",
      "step: 22399, loss: 0.31909194588661194\n",
      "step: 22449, loss: 0.3415643572807312\n",
      "step: 22499, loss: 0.4085952341556549\n",
      "step: 22549, loss: 0.24903063476085663\n",
      "step: 22599, loss: 0.5091755390167236\n",
      "step: 22649, loss: 0.5016937255859375\n",
      "step: 22699, loss: 0.3798113167285919\n",
      "step: 22749, loss: 0.5148010849952698\n",
      "step: 22799, loss: 0.4309900104999542\n",
      "step: 22849, loss: 0.39740386605262756\n",
      "step: 22899, loss: 0.42177820205688477\n",
      "step: 22949, loss: 0.35292428731918335\n",
      "step: 22999, loss: 0.3788086175918579\n",
      "Checkpoint is saved\n",
      "step: 23049, loss: 0.37048208713531494\n",
      "step: 23099, loss: 0.3197609782218933\n",
      "step: 23149, loss: 0.4067641794681549\n",
      "step: 23199, loss: 0.33570927381515503\n",
      "step: 23249, loss: 0.3184082508087158\n",
      "step: 23299, loss: 0.3289898931980133\n",
      "step: 23349, loss: 0.4088912308216095\n",
      "step: 23399, loss: 0.3346133828163147\n",
      "step: 23449, loss: 0.4073280394077301\n",
      "step: 23499, loss: 0.35909491777420044\n",
      "step: 23549, loss: 0.38942140340805054\n",
      "step: 23599, loss: 0.35483092069625854\n",
      "step: 23649, loss: 0.35980215668678284\n",
      "step: 23699, loss: 0.32662543654441833\n",
      "step: 23749, loss: 0.47456130385398865\n",
      "step: 23799, loss: 0.44332799315452576\n",
      "step: 23849, loss: 0.3534936308860779\n",
      "step: 23899, loss: 0.34083229303359985\n",
      "step: 23949, loss: 0.34190720319747925\n",
      "step: 23999, loss: 0.37992891669273376\n",
      "Checkpoint is saved\n",
      "step: 24049, loss: 0.3600630760192871\n",
      "step: 24099, loss: 0.33959752321243286\n",
      "step: 24149, loss: 0.3259003758430481\n",
      "step: 24199, loss: 0.33408600091934204\n",
      "step: 24249, loss: 0.360102117061615\n",
      "step: 24299, loss: 0.29434657096862793\n",
      "step: 24349, loss: 0.32285380363464355\n",
      "step: 24399, loss: 0.2599474787712097\n",
      "step: 24449, loss: 0.32493922114372253\n",
      "step: 24499, loss: 0.36321312189102173\n",
      "step: 24549, loss: 0.31634050607681274\n",
      "step: 24599, loss: 0.4263359606266022\n",
      "step: 24649, loss: 0.535186767578125\n",
      "step: 24699, loss: 0.4908907413482666\n",
      "step: 24749, loss: 0.1895410120487213\n",
      "step: 24799, loss: 0.4268389642238617\n",
      "step: 24849, loss: 0.25977763533592224\n",
      "step: 24899, loss: 0.29202979803085327\n",
      "step: 24949, loss: 0.32853612303733826\n",
      "step: 24999, loss: 0.3474869430065155\n",
      "Checkpoint is saved\n",
      "step: 25049, loss: 0.2994697690010071\n",
      "step: 25099, loss: 0.33234697580337524\n",
      "step: 25149, loss: 0.6574049592018127\n",
      "step: 25199, loss: 0.36319923400878906\n",
      "step: 25249, loss: 0.3216182589530945\n",
      "step: 25299, loss: 0.3705518841743469\n",
      "step: 25349, loss: 0.49385949969291687\n",
      "step: 25399, loss: 0.41325661540031433\n",
      "step: 25449, loss: 0.3869418799877167\n",
      "step: 25499, loss: 0.4214477837085724\n",
      "step: 25549, loss: 0.299164354801178\n",
      "step: 25599, loss: 0.4365841746330261\n",
      "step: 25649, loss: 0.2591440677642822\n",
      "step: 25699, loss: 0.3308446407318115\n",
      "step: 25749, loss: 0.40500205755233765\n",
      "step: 25799, loss: 0.43376243114471436\n",
      "step: 25849, loss: 0.6120707392692566\n",
      "step: 25899, loss: 0.46411773562431335\n",
      "step: 25949, loss: 0.35436493158340454\n",
      "step: 25999, loss: 0.4042322635650635\n",
      "Checkpoint is saved\n",
      "step: 26049, loss: 0.4266355335712433\n",
      "step: 26099, loss: 0.35229235887527466\n",
      "step: 26149, loss: 0.3583340048789978\n",
      "step: 26199, loss: 0.4194810688495636\n",
      "step: 26249, loss: 0.3463188707828522\n",
      "step: 26299, loss: 0.26035255193710327\n",
      "step: 26349, loss: 0.3225208520889282\n",
      "step: 26399, loss: 0.35552477836608887\n",
      "step: 26449, loss: 0.27141135931015015\n",
      "step: 26499, loss: 0.2586332857608795\n",
      "step: 26549, loss: 0.33024507761001587\n",
      "step: 26599, loss: 0.2559891641139984\n",
      "step: 26649, loss: 0.4036880433559418\n",
      "step: 26699, loss: 0.3839447498321533\n",
      "step: 26749, loss: 0.282578706741333\n",
      "step: 26799, loss: 0.40238434076309204\n",
      "step: 26849, loss: 0.37099263072013855\n",
      "step: 26899, loss: 0.24661636352539062\n",
      "step: 26949, loss: 0.33935439586639404\n",
      "step: 26999, loss: 0.2612539529800415\n",
      "Checkpoint is saved\n",
      "step: 27049, loss: 0.45177099108695984\n",
      "step: 27099, loss: 0.3807058334350586\n",
      "step: 27149, loss: 0.3822149932384491\n",
      "step: 27199, loss: 0.47850072383880615\n",
      "step: 27249, loss: 0.2896444797515869\n",
      "step: 27299, loss: 0.2350774109363556\n",
      "step: 27349, loss: 0.36846646666526794\n",
      "step: 27399, loss: 0.4303323030471802\n",
      "step: 27449, loss: 0.34257134795188904\n",
      "step: 27499, loss: 0.3306390643119812\n",
      "step: 27549, loss: 0.3985334634780884\n",
      "step: 27599, loss: 0.2812296748161316\n",
      "step: 27649, loss: 0.24843908846378326\n",
      "step: 27699, loss: 0.21751262247562408\n",
      "step: 27749, loss: 0.4403291940689087\n",
      "step: 27799, loss: 4.641801834106445\n",
      "step: 27849, loss: 0.4208788275718689\n",
      "step: 27899, loss: 0.4236828684806824\n",
      "step: 27949, loss: 0.2801659405231476\n",
      "step: 27999, loss: 0.21003109216690063\n",
      "Checkpoint is saved\n",
      "step: 28049, loss: 0.4690105617046356\n",
      "step: 28099, loss: 0.4447193741798401\n",
      "step: 28149, loss: 0.3990243673324585\n",
      "step: 28199, loss: 0.33547845482826233\n",
      "step: 28249, loss: 0.24946831166744232\n",
      "step: 28299, loss: 0.35969382524490356\n",
      "step: 28349, loss: 0.3883005976676941\n",
      "step: 28399, loss: 0.32918232679367065\n",
      "step: 28449, loss: 0.3094044327735901\n",
      "step: 28499, loss: 0.22961337864398956\n",
      "step: 28549, loss: 0.28724974393844604\n",
      "step: 28599, loss: 0.39032435417175293\n",
      "step: 28649, loss: 0.3959570527076721\n",
      "step: 28699, loss: 0.3624270260334015\n",
      "step: 28749, loss: 0.37850433588027954\n",
      "step: 28799, loss: 0.33021658658981323\n",
      "step: 28849, loss: 0.48496437072753906\n",
      "step: 28899, loss: 0.3524026572704315\n",
      "step: 28949, loss: 0.3178330063819885\n",
      "step: 28999, loss: 0.22221611440181732\n",
      "Checkpoint is saved\n",
      "step: 29049, loss: 0.2703303396701813\n",
      "step: 29099, loss: 0.21648544073104858\n",
      "step: 29149, loss: 0.2923300266265869\n",
      "step: 29199, loss: 0.4104712903499603\n",
      "step: 29249, loss: 0.29832857847213745\n",
      "step: 29299, loss: 0.3762923777103424\n",
      "step: 29349, loss: 0.20946043729782104\n",
      "step: 29399, loss: 0.2657826840877533\n",
      "step: 29449, loss: 0.34054282307624817\n",
      "step: 29499, loss: 0.44749826192855835\n",
      "step: 29549, loss: 0.43634486198425293\n",
      "step: 29599, loss: 0.3180490732192993\n",
      "step: 29649, loss: 0.3945807218551636\n",
      "step: 29699, loss: 0.23626676201820374\n",
      "step: 29749, loss: 0.20641234517097473\n",
      "step: 29799, loss: 0.2546982765197754\n",
      "step: 29849, loss: 0.2422586977481842\n",
      "step: 29899, loss: 0.23071737587451935\n",
      "step: 29949, loss: 0.22978825867176056\n",
      "step: 29999, loss: 0.3969983160495758\n",
      "Checkpoint is saved\n",
      "step: 30049, loss: 0.3384070098400116\n",
      "step: 30099, loss: 0.3500872850418091\n",
      "step: 30149, loss: 0.284858763217926\n",
      "step: 30199, loss: 0.30529820919036865\n",
      "step: 30249, loss: 0.28612226247787476\n",
      "step: 30299, loss: 0.33428969979286194\n",
      "step: 30349, loss: 0.3141089677810669\n",
      "step: 30399, loss: 0.32723209261894226\n",
      "step: 30449, loss: 0.3200448751449585\n",
      "step: 30499, loss: 0.3801180124282837\n",
      "step: 30549, loss: 0.25096118450164795\n",
      "step: 30599, loss: 0.29377442598342896\n",
      "step: 30649, loss: 0.3624272346496582\n",
      "step: 30699, loss: 0.4000140130519867\n",
      "step: 30749, loss: 0.3125208020210266\n",
      "step: 30799, loss: 0.4233464002609253\n",
      "step: 30849, loss: 0.33669015765190125\n",
      "step: 30899, loss: 0.40651607513427734\n",
      "step: 30949, loss: 0.4281861186027527\n",
      "step: 30999, loss: 0.30114492774009705\n",
      "Checkpoint is saved\n",
      "step: 31049, loss: 0.25408774614334106\n",
      "step: 31099, loss: 0.6187573671340942\n",
      "step: 31149, loss: 0.3226161599159241\n",
      "step: 31199, loss: 0.39217716455459595\n",
      "step: 31249, loss: 0.21125352382659912\n",
      "step: 31299, loss: 0.3255142867565155\n",
      "step: 31349, loss: 0.41793060302734375\n",
      "step: 31399, loss: 0.2971314489841461\n",
      "step: 31449, loss: 0.2715389132499695\n",
      "step: 31499, loss: 0.29047030210494995\n",
      "step: 31549, loss: 0.4349723160266876\n",
      "step: 31599, loss: 0.37758493423461914\n",
      "step: 31649, loss: 0.24107901751995087\n",
      "step: 31699, loss: 0.34449493885040283\n",
      "step: 31749, loss: 0.27114272117614746\n",
      "step: 31799, loss: 0.36025646328926086\n",
      "step: 31849, loss: 0.27066653966903687\n",
      "step: 31899, loss: 0.38268834352493286\n",
      "step: 31949, loss: 0.4363366365432739\n",
      "step: 31999, loss: 0.33352309465408325\n",
      "Checkpoint is saved\n",
      "step: 32049, loss: 0.33729076385498047\n",
      "step: 32099, loss: 0.26018229126930237\n",
      "step: 32149, loss: 0.3619478940963745\n",
      "step: 32199, loss: 0.46903666853904724\n",
      "step: 32249, loss: 0.2706892192363739\n",
      "step: 32299, loss: 0.23923802375793457\n",
      "step: 32349, loss: 0.34302544593811035\n",
      "step: 32399, loss: 0.3888821601867676\n",
      "step: 32449, loss: 0.2141081541776657\n",
      "step: 32499, loss: 0.3218410611152649\n",
      "step: 32549, loss: 0.314564049243927\n",
      "step: 32599, loss: 0.23916999995708466\n",
      "step: 32649, loss: 0.28886452317237854\n",
      "step: 32699, loss: 0.30604273080825806\n",
      "step: 32749, loss: 0.36148351430892944\n",
      "step: 32799, loss: 0.2864992618560791\n",
      "step: 32849, loss: 0.29703935980796814\n",
      "step: 32899, loss: 0.3733282685279846\n",
      "step: 32949, loss: 0.29825812578201294\n",
      "step: 32999, loss: 0.27925097942352295\n",
      "Checkpoint is saved\n",
      "step: 33049, loss: 0.3563710153102875\n",
      "step: 33099, loss: 0.36445772647857666\n",
      "step: 33149, loss: 0.324236661195755\n",
      "step: 33199, loss: 0.40564173460006714\n",
      "step: 33249, loss: 0.4418913722038269\n",
      "step: 33299, loss: 0.37289705872535706\n",
      "step: 33349, loss: 0.28564193844795227\n",
      "step: 33399, loss: 0.23837199807167053\n",
      "step: 33449, loss: 0.326394259929657\n",
      "step: 33499, loss: 0.2575485408306122\n",
      "step: 33549, loss: 0.3579586148262024\n",
      "step: 33599, loss: 0.4127175509929657\n",
      "step: 33649, loss: 0.3218587040901184\n",
      "step: 33699, loss: 0.4578712582588196\n",
      "step: 33749, loss: 0.3049100637435913\n",
      "step: 33799, loss: 0.23576058447360992\n",
      "step: 33849, loss: 0.2600189745426178\n",
      "step: 33899, loss: 0.37314367294311523\n",
      "step: 33949, loss: 0.2617190480232239\n",
      "step: 33999, loss: 0.3319280743598938\n",
      "Checkpoint is saved\n",
      "step: 34049, loss: 0.28169310092926025\n",
      "step: 34099, loss: 0.3760799169540405\n",
      "step: 34149, loss: 0.24015960097312927\n",
      "step: 34199, loss: 0.4225013256072998\n",
      "step: 34249, loss: 0.2996947467327118\n",
      "step: 34299, loss: 0.2789066731929779\n",
      "step: 34349, loss: 0.3500552773475647\n",
      "step: 34399, loss: 0.25242409110069275\n",
      "step: 34449, loss: 0.318779319524765\n",
      "step: 34499, loss: 0.3349458873271942\n",
      "step: 34549, loss: 0.20917877554893494\n",
      "step: 34599, loss: 0.3750709891319275\n",
      "step: 34649, loss: 0.3287200629711151\n",
      "step: 34699, loss: 0.27528613805770874\n",
      "step: 34749, loss: 0.3463999330997467\n",
      "step: 34799, loss: 0.2261192947626114\n",
      "step: 34849, loss: 0.2543228566646576\n",
      "step: 34899, loss: 0.39378878474235535\n",
      "step: 34949, loss: 0.32199525833129883\n",
      "step: 34999, loss: 0.29856863617897034\n",
      "Checkpoint is saved\n",
      "step: 35049, loss: 0.4058331549167633\n",
      "step: 35099, loss: 0.2656569480895996\n",
      "step: 35149, loss: 0.21010711789131165\n",
      "step: 35199, loss: 0.35619211196899414\n",
      "step: 35249, loss: 0.3838937282562256\n",
      "step: 35299, loss: 0.23895803093910217\n",
      "step: 35349, loss: 0.1768326312303543\n",
      "step: 35399, loss: 0.27016890048980713\n",
      "step: 35449, loss: 0.3701007068157196\n",
      "step: 35499, loss: 0.2808915674686432\n",
      "step: 35549, loss: 0.3808130919933319\n",
      "step: 35599, loss: 0.25133007764816284\n",
      "step: 35649, loss: 0.35152319073677063\n",
      "step: 35699, loss: 0.3077869713306427\n",
      "step: 35749, loss: 0.2591927647590637\n",
      "step: 35799, loss: 0.45556172728538513\n",
      "step: 35849, loss: 0.28722697496414185\n",
      "step: 35899, loss: 0.227590873837471\n",
      "step: 35949, loss: 0.4955177307128906\n",
      "step: 35999, loss: 0.2844788432121277\n",
      "Checkpoint is saved\n",
      "step: 36049, loss: 0.22270961105823517\n",
      "step: 36099, loss: 0.3997564911842346\n",
      "step: 36149, loss: 0.33037513494491577\n",
      "step: 36199, loss: 0.31232398748397827\n",
      "step: 36249, loss: 0.24952800571918488\n",
      "step: 36299, loss: 0.30419328808784485\n",
      "step: 36349, loss: 0.3351760506629944\n",
      "step: 36399, loss: 0.2990271747112274\n",
      "step: 36449, loss: 0.36745017766952515\n",
      "step: 36499, loss: 0.3447866439819336\n",
      "step: 36549, loss: 0.31577008962631226\n",
      "step: 36599, loss: 0.3387792110443115\n",
      "step: 36649, loss: 0.24374572932720184\n",
      "step: 36699, loss: 0.22924281656742096\n",
      "step: 36749, loss: 0.2967199683189392\n",
      "step: 36799, loss: 0.3000478744506836\n",
      "step: 36849, loss: 0.31758785247802734\n",
      "step: 36899, loss: 0.2756233811378479\n",
      "step: 36949, loss: 0.3968725800514221\n",
      "step: 36999, loss: 0.2902016043663025\n",
      "Checkpoint is saved\n",
      "step: 37049, loss: 0.2943316102027893\n",
      "step: 37099, loss: 0.35608357191085815\n",
      "step: 37149, loss: 0.2279416024684906\n",
      "step: 37199, loss: 0.25944459438323975\n",
      "step: 37249, loss: 0.2507505416870117\n",
      "step: 37299, loss: 0.2727716863155365\n",
      "step: 37349, loss: 0.30386874079704285\n",
      "step: 37399, loss: 0.3118276596069336\n",
      "step: 37449, loss: 0.24307766556739807\n",
      "step: 37499, loss: 0.3404390513896942\n",
      "step: 37549, loss: 0.26262086629867554\n",
      "step: 37599, loss: 0.2381875067949295\n",
      "step: 37649, loss: 0.34441035985946655\n",
      "step: 37699, loss: 0.3025251030921936\n",
      "step: 37749, loss: 0.26254114508628845\n",
      "step: 37799, loss: 0.2143145501613617\n",
      "step: 37849, loss: 0.25460538268089294\n",
      "step: 37899, loss: 0.32565709948539734\n",
      "step: 37949, loss: 0.2767906188964844\n",
      "step: 37999, loss: 0.26342764496803284\n",
      "Checkpoint is saved\n",
      "step: 38049, loss: 0.26935625076293945\n",
      "step: 38099, loss: 0.2659294009208679\n",
      "step: 38149, loss: 0.34503576159477234\n",
      "step: 38199, loss: 0.28022313117980957\n",
      "step: 38249, loss: 0.2948176860809326\n",
      "step: 38299, loss: 0.2745664119720459\n",
      "step: 38349, loss: 0.20375186204910278\n",
      "step: 38399, loss: 0.3085215091705322\n",
      "step: 38449, loss: 0.26469361782073975\n",
      "step: 38499, loss: 0.32613635063171387\n",
      "step: 38549, loss: 0.24367569386959076\n",
      "step: 38599, loss: 0.2438252568244934\n",
      "step: 38649, loss: 0.31142038106918335\n",
      "step: 38699, loss: 0.3823464512825012\n",
      "step: 38749, loss: 0.18905875086784363\n",
      "step: 38799, loss: 0.2927194833755493\n",
      "step: 38849, loss: 0.3155699372291565\n",
      "step: 38899, loss: 0.40934452414512634\n",
      "step: 38949, loss: 0.4044388234615326\n",
      "step: 38999, loss: 0.16661286354064941\n",
      "Checkpoint is saved\n",
      "step: 39049, loss: 0.33351707458496094\n",
      "step: 39099, loss: 0.26848602294921875\n",
      "step: 39149, loss: 0.24513016641139984\n",
      "step: 39199, loss: 0.4638650715351105\n",
      "step: 39249, loss: 0.3872922956943512\n",
      "step: 39299, loss: 0.3511620759963989\n",
      "step: 39349, loss: 0.21265161037445068\n",
      "step: 39399, loss: 0.2443239390850067\n",
      "step: 39449, loss: 0.4582146108150482\n",
      "step: 39499, loss: 0.3272194266319275\n",
      "step: 39549, loss: 0.2898668646812439\n",
      "step: 39599, loss: 0.2111104130744934\n",
      "step: 39649, loss: 0.3671920895576477\n",
      "step: 39699, loss: 0.46752530336380005\n",
      "step: 39749, loss: 0.19124379754066467\n",
      "step: 39799, loss: 0.32538020610809326\n",
      "step: 39849, loss: 0.30514976382255554\n",
      "step: 39899, loss: 0.38167399168014526\n",
      "step: 39949, loss: 0.2696355879306793\n",
      "step: 39999, loss: 0.3810558319091797\n",
      "Checkpoint is saved\n",
      "Training time for 40000 steps: 4533.167253494263s\n"
     ]
    }
   ],
   "source": [
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 50 == 49 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 1000 == 999:\n",
    "            saver.save(sess, './checkpoints/', global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAGTCAYAAAAfltc1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2800389acf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_test = {'X':X_test, 'Y':Y_test}\n",
    "pickle.dump(encoded_test, open(\"./encoded_test.p\", mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

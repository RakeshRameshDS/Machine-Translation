{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"./data.p\"\n",
    "X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['के', '', 'में', 'है', 'की', 'और', 'से', 'को', 'का', 'हैं', 'कि', 'पर', 'एक', 'नहीं', 'लिए', 'यह', 'भी', 'इस', 'कर', 'ने', 'हो', '।', 'ही', 'करने', 'जो', 'तो', 'किया', 'या', 'था', 'आप']\n",
      "['the', '', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for', 'it', 'this', 'on', 'you', 'was', 'as', 'are', 'with', 'be', 'not', 'by', 'or', 'he', 'from', 'his', 'have', 'but', 'at', 'an', 'which']\n"
     ]
    }
   ],
   "source": [
    "print(l1_vocab[:30])\n",
    "print(l2_vocab[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "# data padding\n",
    "def data_padding(x, y, length = 20):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [l1_word2idx['<pad>']]\n",
    "        y[i] = [l2_word2idx['<go>']] + y[i] + [l2_word2idx['<eos>']] + (length-len(y[i])) * [l2_word2idx['<pad>']]\n",
    "\n",
    "data_padding(X, Y)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "l1_vocab_size = len(l1_vocab) + 2 # + <pad>, <ukn>\n",
    "l2_vocab_size = len(l2_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{0}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{0}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [l2_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [l2_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = l1_vocab_size,\n",
    "                                            num_decoder_symbols = l2_vocab_size,\n",
    "                                            embedding_size = 80,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(logits, labels):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = l2_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.nn.seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = l2_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == l2_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(l2_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 3e-3\n",
    "batch_size = 8\n",
    "steps = 40000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.993).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 9.588481903076172\n",
      "step: 49, loss: 7.211920738220215\n",
      "step: 99, loss: 6.195350646972656\n",
      "step: 149, loss: 6.026942253112793\n",
      "step: 199, loss: 5.360556602478027\n",
      "step: 249, loss: 5.569120407104492\n",
      "step: 299, loss: 5.033092021942139\n",
      "step: 349, loss: 5.140023231506348\n",
      "step: 399, loss: 5.110561370849609\n",
      "step: 449, loss: 4.270654201507568\n",
      "step: 499, loss: 4.948354244232178\n",
      "step: 549, loss: 4.8515729904174805\n",
      "step: 599, loss: 4.565873146057129\n",
      "step: 649, loss: 4.715709209442139\n",
      "step: 699, loss: 4.538848400115967\n",
      "step: 749, loss: 4.746813774108887\n",
      "step: 799, loss: 4.277988910675049\n",
      "step: 849, loss: 3.7922844886779785\n",
      "step: 899, loss: 4.518959999084473\n",
      "step: 949, loss: 4.202607154846191\n",
      "step: 999, loss: 4.135334014892578\n",
      "Checkpoint is saved\n",
      "step: 1049, loss: 4.151702404022217\n",
      "step: 1099, loss: 3.954167127609253\n",
      "step: 1149, loss: 4.039559364318848\n",
      "step: 1199, loss: 4.147208213806152\n",
      "step: 1249, loss: 3.913249969482422\n",
      "step: 1299, loss: 3.599726915359497\n",
      "step: 1349, loss: 3.5817770957946777\n",
      "step: 1399, loss: 3.4706547260284424\n",
      "step: 1449, loss: 3.794600486755371\n",
      "step: 1499, loss: 3.5652804374694824\n",
      "step: 1549, loss: 3.2854905128479004\n",
      "step: 1599, loss: 3.307371139526367\n",
      "step: 1649, loss: 3.026188850402832\n",
      "step: 1699, loss: 2.8489317893981934\n",
      "step: 1749, loss: 2.974621295928955\n",
      "step: 1799, loss: 2.8914034366607666\n",
      "step: 1849, loss: 3.119013786315918\n",
      "step: 1899, loss: 2.989283323287964\n",
      "step: 1949, loss: 3.0370912551879883\n",
      "step: 1999, loss: 2.9809131622314453\n",
      "Checkpoint is saved\n",
      "step: 2049, loss: 2.9293317794799805\n",
      "step: 2099, loss: 2.9103479385375977\n",
      "step: 2149, loss: 2.930056571960449\n",
      "step: 2199, loss: 3.182640552520752\n",
      "step: 2249, loss: 2.7027196884155273\n",
      "step: 2299, loss: 2.9354665279388428\n",
      "step: 2349, loss: 2.793996572494507\n",
      "step: 2399, loss: 2.6300318241119385\n",
      "step: 2449, loss: 2.470041275024414\n",
      "step: 2499, loss: 2.5593690872192383\n",
      "step: 2549, loss: 2.4863767623901367\n",
      "step: 2599, loss: 2.6543405055999756\n",
      "step: 2649, loss: 2.7176403999328613\n",
      "step: 2699, loss: 2.418048620223999\n",
      "step: 2749, loss: 2.367788791656494\n",
      "step: 2799, loss: 2.4183478355407715\n",
      "step: 2849, loss: 2.2283759117126465\n",
      "step: 2899, loss: 2.9620203971862793\n",
      "step: 2949, loss: 2.4271206855773926\n",
      "step: 2999, loss: 2.2384190559387207\n",
      "Checkpoint is saved\n",
      "step: 3049, loss: 2.1795730590820312\n",
      "step: 3099, loss: 2.424811601638794\n",
      "step: 3149, loss: 2.2930264472961426\n",
      "step: 3199, loss: 2.408223867416382\n",
      "step: 3249, loss: 2.3629636764526367\n",
      "step: 3299, loss: 2.3578548431396484\n",
      "step: 3349, loss: 2.381105661392212\n",
      "step: 3399, loss: 2.2365527153015137\n",
      "step: 3449, loss: 1.7316259145736694\n",
      "step: 3499, loss: 2.080470561981201\n",
      "step: 3549, loss: 2.227996349334717\n",
      "step: 3599, loss: 2.386371612548828\n",
      "step: 3649, loss: 1.9684154987335205\n",
      "step: 3699, loss: 1.8216251134872437\n",
      "step: 3749, loss: 1.8451788425445557\n",
      "step: 3799, loss: 1.8520078659057617\n",
      "step: 3849, loss: 1.8555643558502197\n",
      "step: 3899, loss: 1.9200983047485352\n",
      "step: 3949, loss: 1.802176833152771\n",
      "step: 3999, loss: 1.8529810905456543\n",
      "Checkpoint is saved\n",
      "step: 4049, loss: 2.221066474914551\n",
      "step: 4099, loss: 1.8071269989013672\n",
      "step: 4149, loss: 1.9501514434814453\n",
      "step: 4199, loss: 1.9426542520523071\n",
      "step: 4249, loss: 1.840987205505371\n",
      "step: 4299, loss: 1.4719321727752686\n",
      "step: 4349, loss: 1.9021844863891602\n",
      "step: 4399, loss: 1.974860668182373\n",
      "step: 4449, loss: 1.7896844148635864\n",
      "step: 4499, loss: 1.8685166835784912\n",
      "step: 4549, loss: 1.6422851085662842\n",
      "step: 4599, loss: 1.5484955310821533\n",
      "step: 4649, loss: 1.539595365524292\n",
      "step: 4699, loss: 1.5594303607940674\n",
      "step: 4749, loss: 1.728101134300232\n",
      "step: 4799, loss: 1.6832685470581055\n",
      "step: 4849, loss: 1.7522039413452148\n",
      "step: 4899, loss: 1.8804841041564941\n",
      "step: 4949, loss: 1.6279611587524414\n",
      "step: 4999, loss: 1.5951495170593262\n",
      "Checkpoint is saved\n",
      "step: 5049, loss: 1.5322377681732178\n",
      "step: 5099, loss: 1.5774097442626953\n",
      "step: 5149, loss: 1.3286044597625732\n",
      "step: 5199, loss: 1.6562803983688354\n",
      "step: 5249, loss: 1.4910037517547607\n",
      "step: 5299, loss: 1.6428182125091553\n",
      "step: 5349, loss: 1.3612048625946045\n",
      "step: 5399, loss: 1.765721082687378\n",
      "step: 5449, loss: 1.465585708618164\n",
      "step: 5499, loss: 1.3173600435256958\n",
      "step: 5549, loss: 1.6467875242233276\n",
      "step: 5599, loss: 1.7502139806747437\n",
      "step: 5649, loss: 1.4443185329437256\n",
      "step: 5699, loss: 1.4503761529922485\n",
      "step: 5749, loss: 1.677168369293213\n",
      "step: 5799, loss: 1.414179801940918\n",
      "step: 5849, loss: 1.6892563104629517\n",
      "step: 5899, loss: 1.200856328010559\n",
      "step: 5949, loss: 1.3852335214614868\n",
      "step: 5999, loss: 1.511749267578125\n",
      "Checkpoint is saved\n",
      "step: 6049, loss: 1.5264649391174316\n",
      "step: 6099, loss: 1.4295387268066406\n",
      "step: 6149, loss: 1.6270098686218262\n",
      "step: 6199, loss: 1.2618730068206787\n",
      "step: 6249, loss: 1.5303308963775635\n",
      "step: 6299, loss: 1.4306588172912598\n",
      "step: 6349, loss: 1.3554900884628296\n",
      "step: 6399, loss: 1.3060424327850342\n",
      "step: 6449, loss: 1.4974784851074219\n",
      "step: 6499, loss: 1.208458662033081\n",
      "step: 6549, loss: 1.3159983158111572\n",
      "step: 6599, loss: 1.618532657623291\n",
      "step: 6649, loss: 1.2575733661651611\n",
      "step: 6699, loss: 1.452457070350647\n",
      "step: 6749, loss: 1.6759932041168213\n",
      "step: 6799, loss: 1.3384929895401\n",
      "step: 6849, loss: 1.552800178527832\n",
      "step: 6899, loss: 1.158740758895874\n",
      "step: 6949, loss: 1.5754822492599487\n",
      "step: 6999, loss: 1.2764828205108643\n",
      "Checkpoint is saved\n",
      "step: 7049, loss: 1.3714258670806885\n",
      "step: 7099, loss: 1.3922361135482788\n",
      "step: 7149, loss: 1.2774386405944824\n",
      "step: 7199, loss: 0.9710763692855835\n",
      "step: 7249, loss: 1.114452600479126\n",
      "step: 7299, loss: 1.2160613536834717\n",
      "step: 7349, loss: 1.1226954460144043\n",
      "step: 7399, loss: 1.4167346954345703\n",
      "step: 7449, loss: 1.276818037033081\n",
      "step: 7499, loss: 1.4862266778945923\n",
      "step: 7549, loss: 1.02207612991333\n",
      "step: 7599, loss: 1.099804162979126\n",
      "step: 7649, loss: 1.4284491539001465\n",
      "step: 7699, loss: 1.2300565242767334\n",
      "step: 7749, loss: 1.053180456161499\n",
      "step: 7799, loss: 1.1580744981765747\n",
      "step: 7849, loss: 1.3034754991531372\n",
      "step: 7899, loss: 1.2775866985321045\n",
      "step: 7949, loss: 1.3309139013290405\n",
      "step: 7999, loss: 1.127763032913208\n",
      "Checkpoint is saved\n",
      "step: 8049, loss: 1.2367589473724365\n",
      "step: 8099, loss: 1.1379984617233276\n",
      "step: 8149, loss: 1.2820279598236084\n",
      "step: 8199, loss: 1.2538373470306396\n",
      "step: 8249, loss: 1.1366615295410156\n",
      "step: 8299, loss: 1.1322312355041504\n",
      "step: 8349, loss: 1.2844592332839966\n",
      "step: 8399, loss: 1.2279828786849976\n",
      "step: 8449, loss: 1.220890760421753\n",
      "step: 8499, loss: 1.1251839399337769\n",
      "step: 8549, loss: 1.3817806243896484\n",
      "step: 8599, loss: 1.24578857421875\n",
      "step: 8649, loss: 1.0733798742294312\n",
      "step: 8699, loss: 0.9023622274398804\n",
      "step: 8749, loss: 0.9903565645217896\n",
      "step: 8799, loss: 1.1059812307357788\n",
      "step: 8849, loss: 1.065349817276001\n",
      "step: 8899, loss: 1.2479304075241089\n",
      "step: 8949, loss: 1.3199572563171387\n",
      "step: 8999, loss: 1.1198763847351074\n",
      "Checkpoint is saved\n",
      "step: 9049, loss: 1.370784044265747\n",
      "step: 9099, loss: 1.1432605981826782\n",
      "step: 9149, loss: 1.0899982452392578\n",
      "step: 9199, loss: 1.1665843725204468\n",
      "step: 9249, loss: 1.0714213848114014\n",
      "step: 9299, loss: 0.958509087562561\n",
      "step: 9349, loss: 0.8404427170753479\n",
      "step: 9399, loss: 0.9491167068481445\n",
      "step: 9449, loss: 0.9938419461250305\n",
      "step: 9499, loss: 0.9832081198692322\n",
      "step: 9549, loss: 1.0622341632843018\n",
      "step: 9599, loss: 0.9695078134536743\n",
      "step: 9649, loss: 1.12611985206604\n",
      "step: 9699, loss: 1.0062240362167358\n",
      "step: 9749, loss: 1.0634784698486328\n",
      "step: 9799, loss: 0.8003195524215698\n",
      "step: 9849, loss: 0.7922813296318054\n",
      "step: 9899, loss: 1.0010648965835571\n",
      "step: 9949, loss: 1.276646614074707\n",
      "step: 9999, loss: 1.02989661693573\n",
      "Checkpoint is saved\n",
      "step: 10049, loss: 1.0283130407333374\n",
      "step: 10099, loss: 0.8314408659934998\n",
      "step: 10149, loss: 1.095780611038208\n",
      "step: 10199, loss: 0.7974203824996948\n",
      "step: 10249, loss: 0.993313729763031\n",
      "step: 10299, loss: 0.8284556865692139\n",
      "step: 10349, loss: 1.0720064640045166\n",
      "step: 10399, loss: 0.8734416365623474\n",
      "step: 10449, loss: 1.1606967449188232\n",
      "step: 10499, loss: 1.021630048751831\n",
      "step: 10549, loss: 1.2555556297302246\n",
      "step: 10599, loss: 1.1933320760726929\n",
      "step: 10649, loss: 1.0275307893753052\n",
      "step: 10699, loss: 0.6599174737930298\n",
      "step: 10749, loss: 1.0019112825393677\n",
      "step: 10799, loss: 0.8415998220443726\n",
      "step: 10849, loss: 0.8522677421569824\n",
      "step: 10899, loss: 0.9612674713134766\n",
      "step: 10949, loss: 1.0824916362762451\n",
      "step: 10999, loss: 0.8929619193077087\n",
      "Checkpoint is saved\n",
      "step: 11049, loss: 1.169624924659729\n",
      "step: 11099, loss: 1.009294033050537\n",
      "step: 11149, loss: 1.160718321800232\n",
      "step: 11199, loss: 0.9349284172058105\n",
      "step: 11249, loss: 0.836219310760498\n",
      "step: 11299, loss: 0.7841448187828064\n",
      "step: 11349, loss: 0.8016828894615173\n",
      "step: 11399, loss: 0.9826222658157349\n",
      "step: 11449, loss: 1.071907877922058\n",
      "step: 11499, loss: 0.6553698778152466\n",
      "step: 11549, loss: 0.9595720767974854\n",
      "step: 11599, loss: 0.7876765131950378\n",
      "step: 11649, loss: 0.7279340028762817\n",
      "step: 11699, loss: 0.7661511898040771\n",
      "step: 11749, loss: 0.9540594816207886\n",
      "step: 11799, loss: 0.9198561906814575\n",
      "step: 11849, loss: 0.683107316493988\n",
      "step: 11899, loss: 0.9801812171936035\n",
      "step: 11949, loss: 1.1275783777236938\n",
      "step: 11999, loss: 0.7920063734054565\n",
      "Checkpoint is saved\n",
      "step: 12049, loss: 0.9628822803497314\n",
      "step: 12099, loss: 0.72709059715271\n",
      "step: 12149, loss: 0.9322723150253296\n",
      "step: 12199, loss: 0.8248013257980347\n",
      "step: 12249, loss: 0.8432532548904419\n",
      "step: 12299, loss: 0.845706582069397\n",
      "step: 12349, loss: 0.8621203303337097\n",
      "step: 12399, loss: 1.0392571687698364\n",
      "step: 12449, loss: 0.821184515953064\n",
      "step: 12499, loss: 0.8942438364028931\n",
      "step: 12549, loss: 0.7774296402931213\n",
      "step: 12599, loss: 0.7396873235702515\n",
      "step: 12649, loss: 0.6789556741714478\n",
      "step: 12699, loss: 0.9186932444572449\n",
      "step: 12749, loss: 0.7630813121795654\n",
      "step: 12799, loss: 0.8855820894241333\n",
      "step: 12849, loss: 0.8303775787353516\n",
      "step: 12899, loss: 0.9185755252838135\n",
      "step: 12949, loss: 0.9192760586738586\n",
      "step: 12999, loss: 0.7603519558906555\n",
      "Checkpoint is saved\n",
      "step: 13049, loss: 0.6793931126594543\n",
      "step: 13099, loss: 1.1375831365585327\n",
      "step: 13149, loss: 0.9003685712814331\n",
      "step: 13199, loss: 0.6924332976341248\n",
      "step: 13249, loss: 0.6175023913383484\n",
      "step: 13299, loss: 0.48934227228164673\n",
      "step: 13349, loss: 0.9932352900505066\n",
      "step: 13399, loss: 0.8845064640045166\n",
      "step: 13449, loss: 0.9869559407234192\n",
      "step: 13499, loss: 0.8539307117462158\n",
      "step: 13549, loss: 0.8446840643882751\n",
      "step: 13599, loss: 0.8792595863342285\n",
      "step: 13649, loss: 0.7582862973213196\n",
      "step: 13699, loss: 0.6435832381248474\n",
      "step: 13749, loss: 0.7046869993209839\n",
      "step: 13799, loss: 0.6821715831756592\n",
      "step: 13849, loss: 0.7475768327713013\n",
      "step: 13899, loss: 0.7123107314109802\n",
      "step: 13949, loss: 0.5609521865844727\n",
      "step: 13999, loss: 0.7047919034957886\n",
      "Checkpoint is saved\n",
      "step: 14049, loss: 0.674048900604248\n",
      "step: 14099, loss: 0.6703140735626221\n",
      "step: 14149, loss: 0.7735058069229126\n",
      "step: 14199, loss: 0.7863907217979431\n",
      "step: 14249, loss: 0.706366240978241\n",
      "step: 14299, loss: 0.7902008891105652\n",
      "step: 14349, loss: 0.7387393712997437\n",
      "step: 14399, loss: 0.6855147480964661\n",
      "step: 14449, loss: 0.7954202890396118\n",
      "step: 14499, loss: 0.8877220749855042\n",
      "step: 14549, loss: 0.8083697557449341\n",
      "step: 14599, loss: 0.8301577568054199\n",
      "step: 14649, loss: 0.701668918132782\n",
      "step: 14699, loss: 0.7904691696166992\n",
      "step: 14749, loss: 0.8816680908203125\n",
      "step: 14799, loss: 0.8038269281387329\n",
      "step: 14849, loss: 0.49831104278564453\n",
      "step: 14899, loss: 0.7515709400177002\n",
      "step: 14949, loss: 0.9341641068458557\n",
      "step: 14999, loss: 0.7251280546188354\n",
      "Checkpoint is saved\n",
      "step: 15049, loss: 0.6973738670349121\n",
      "step: 15099, loss: 0.8040916919708252\n",
      "step: 15149, loss: 0.718261182308197\n",
      "step: 15199, loss: 0.6871193051338196\n",
      "step: 15249, loss: 0.7624233961105347\n",
      "step: 15299, loss: 0.5389194488525391\n",
      "step: 15349, loss: 0.6891098022460938\n",
      "step: 15399, loss: 0.7885768413543701\n",
      "step: 15449, loss: 0.5952681303024292\n",
      "step: 15499, loss: 0.6769496202468872\n",
      "step: 15549, loss: 0.5886012315750122\n",
      "step: 15599, loss: 0.4911652207374573\n",
      "step: 15649, loss: 0.9230693578720093\n",
      "step: 15699, loss: 0.7776651382446289\n",
      "step: 15749, loss: 0.5702109336853027\n",
      "step: 15799, loss: 0.7317422032356262\n",
      "step: 15849, loss: 0.8865299224853516\n",
      "step: 15899, loss: 0.6336076855659485\n",
      "step: 15949, loss: 0.5955044031143188\n",
      "step: 15999, loss: 0.5563381910324097\n",
      "Checkpoint is saved\n",
      "step: 16049, loss: 0.5890231132507324\n",
      "step: 16099, loss: 0.6357492208480835\n",
      "step: 16149, loss: 0.7416808605194092\n",
      "step: 16199, loss: 0.709805965423584\n",
      "step: 16249, loss: 0.6601308584213257\n",
      "step: 16299, loss: 0.7330718040466309\n",
      "step: 16349, loss: 0.6958976984024048\n",
      "step: 16399, loss: 0.6142318844795227\n",
      "step: 16449, loss: 0.6568509340286255\n",
      "step: 16499, loss: 0.4930112957954407\n",
      "step: 16549, loss: 0.6144812107086182\n",
      "step: 16599, loss: 0.5933288335800171\n",
      "step: 16649, loss: 0.5811280012130737\n",
      "step: 16699, loss: 0.41732466220855713\n",
      "step: 16749, loss: 0.6692148447036743\n",
      "step: 16799, loss: 0.7064348459243774\n",
      "step: 16849, loss: 0.6213953495025635\n",
      "step: 16899, loss: 0.5512915253639221\n",
      "step: 16949, loss: 0.7263064384460449\n",
      "step: 16999, loss: 0.6888264417648315\n",
      "Checkpoint is saved\n",
      "step: 17049, loss: 0.5882716178894043\n",
      "step: 17099, loss: 0.6825810074806213\n",
      "step: 17149, loss: 0.7509512901306152\n",
      "step: 17199, loss: 0.7933057546615601\n",
      "step: 17249, loss: 0.786566972732544\n",
      "step: 17299, loss: 0.631510317325592\n",
      "step: 17349, loss: 0.780199408531189\n",
      "step: 17399, loss: 0.5515881776809692\n",
      "step: 17449, loss: 0.6094305515289307\n",
      "step: 17499, loss: 0.548108696937561\n",
      "step: 17549, loss: 0.7712876796722412\n",
      "step: 17599, loss: 0.4647340476512909\n",
      "step: 17649, loss: 0.7323192954063416\n",
      "step: 17699, loss: 0.6645429134368896\n",
      "step: 17749, loss: 0.5589849948883057\n",
      "step: 17799, loss: 0.47778192162513733\n",
      "step: 17849, loss: 0.7017607688903809\n",
      "step: 17899, loss: 0.5767189264297485\n",
      "step: 17949, loss: 0.646902322769165\n",
      "step: 17999, loss: 0.576789140701294\n",
      "Checkpoint is saved\n",
      "step: 18049, loss: 0.7380915880203247\n",
      "step: 18099, loss: 0.5615178942680359\n",
      "step: 18149, loss: 0.5984732508659363\n",
      "step: 18199, loss: 0.5408250689506531\n",
      "step: 18249, loss: 0.5649733543395996\n",
      "step: 18299, loss: 0.623786449432373\n",
      "step: 18349, loss: 0.634581983089447\n",
      "step: 18399, loss: 0.6125410795211792\n",
      "step: 18449, loss: 0.663104236125946\n",
      "step: 18499, loss: 0.7080841064453125\n",
      "step: 18549, loss: 0.5140650272369385\n",
      "step: 18599, loss: 0.7605385780334473\n",
      "step: 18649, loss: 0.6436639428138733\n",
      "step: 18699, loss: 0.7496750354766846\n",
      "step: 18749, loss: 0.7411980628967285\n",
      "step: 18799, loss: 0.5451009273529053\n",
      "step: 18849, loss: 0.4854368269443512\n",
      "step: 18899, loss: 0.3580274283885956\n",
      "step: 18949, loss: 0.5312985777854919\n",
      "step: 18999, loss: 0.7373044490814209\n",
      "Checkpoint is saved\n",
      "step: 19049, loss: 0.6109535694122314\n",
      "step: 19099, loss: 0.4951778054237366\n",
      "step: 19149, loss: 0.44757071137428284\n",
      "step: 19199, loss: 0.4649793803691864\n",
      "step: 19249, loss: 0.5532137155532837\n",
      "step: 19299, loss: 0.45931363105773926\n",
      "step: 19349, loss: 0.4664871096611023\n",
      "step: 19399, loss: 0.5899063944816589\n",
      "step: 19449, loss: 0.48315179347991943\n",
      "step: 19499, loss: 0.5298783779144287\n",
      "step: 19549, loss: 0.543342113494873\n",
      "step: 19599, loss: 0.4909781515598297\n",
      "step: 19649, loss: 0.678885817527771\n",
      "step: 19699, loss: 0.6760818958282471\n",
      "step: 19749, loss: 0.5740369558334351\n",
      "step: 19799, loss: 0.7796480655670166\n",
      "step: 19849, loss: 0.619724452495575\n",
      "step: 19899, loss: 0.5729454755783081\n",
      "step: 19949, loss: 0.6222144365310669\n",
      "step: 19999, loss: 0.7576757073402405\n",
      "Checkpoint is saved\n",
      "step: 20049, loss: 0.42402154207229614\n",
      "step: 20099, loss: 0.6751099824905396\n",
      "step: 20149, loss: 0.557568371295929\n",
      "step: 20199, loss: 0.5121163129806519\n",
      "step: 20249, loss: 0.3223300874233246\n",
      "step: 20299, loss: 0.4912346601486206\n",
      "step: 20349, loss: 0.45020580291748047\n",
      "step: 20399, loss: 0.43592095375061035\n",
      "step: 20449, loss: 0.4534665644168854\n",
      "step: 20499, loss: 0.49460944533348083\n",
      "step: 20549, loss: 0.6033074855804443\n",
      "step: 20599, loss: 0.5509229898452759\n",
      "step: 20649, loss: 0.3903425931930542\n",
      "step: 20699, loss: 0.4687619209289551\n",
      "step: 20749, loss: 0.4101618528366089\n",
      "step: 20799, loss: 0.5688193440437317\n",
      "step: 20849, loss: 0.5977210402488708\n",
      "step: 20899, loss: 0.39888888597488403\n",
      "step: 20949, loss: 0.6049250960350037\n",
      "step: 20999, loss: 0.5702323913574219\n",
      "Checkpoint is saved\n",
      "step: 21049, loss: 0.5305395722389221\n",
      "step: 21099, loss: 0.4655756950378418\n",
      "step: 21149, loss: 0.49568355083465576\n",
      "step: 21199, loss: 0.5480964183807373\n",
      "step: 21249, loss: 0.5993096828460693\n",
      "step: 21299, loss: 0.47454968094825745\n",
      "step: 21349, loss: 0.48086273670196533\n",
      "step: 21399, loss: 0.5104435682296753\n",
      "step: 21449, loss: 0.4510430693626404\n",
      "step: 21499, loss: 0.3895583748817444\n",
      "step: 21549, loss: 0.5384989976882935\n",
      "step: 21599, loss: 0.4102481007575989\n",
      "step: 21649, loss: 0.42518138885498047\n",
      "step: 21699, loss: 0.5764172673225403\n",
      "step: 21749, loss: 0.39282459020614624\n",
      "step: 21799, loss: 0.4552702307701111\n",
      "step: 21849, loss: 0.6494470834732056\n",
      "step: 21899, loss: 0.49539127945899963\n",
      "step: 21949, loss: 0.40529143810272217\n",
      "step: 21999, loss: 0.49922457337379456\n",
      "Checkpoint is saved\n",
      "step: 22049, loss: 0.6056653261184692\n",
      "step: 22099, loss: 0.3458377718925476\n",
      "step: 22149, loss: 0.49266478419303894\n",
      "step: 22199, loss: 0.5805202126502991\n",
      "step: 22249, loss: 0.5050798058509827\n",
      "step: 22299, loss: 0.42077451944351196\n",
      "step: 22349, loss: 0.4910010099411011\n",
      "step: 22399, loss: 0.4945930540561676\n",
      "step: 22449, loss: 0.5640966892242432\n",
      "step: 22499, loss: 0.461006224155426\n",
      "step: 22549, loss: 0.34599754214286804\n",
      "step: 22599, loss: 0.5376911163330078\n",
      "step: 22649, loss: 0.5323042273521423\n",
      "step: 22699, loss: 0.4735174775123596\n",
      "step: 22749, loss: 0.5726319551467896\n",
      "step: 22799, loss: 0.3846588730812073\n",
      "step: 22849, loss: 0.43921685218811035\n",
      "step: 22899, loss: 0.4567797780036926\n",
      "step: 22949, loss: 0.5323371887207031\n",
      "step: 22999, loss: 0.654382586479187\n",
      "Checkpoint is saved\n",
      "step: 23049, loss: 0.47380387783050537\n",
      "step: 23099, loss: 0.3377159535884857\n",
      "step: 23149, loss: 0.47272515296936035\n",
      "step: 23199, loss: 0.46635937690734863\n",
      "step: 23249, loss: 0.4791809022426605\n",
      "step: 23299, loss: 0.5463220477104187\n",
      "step: 23349, loss: 0.44496291875839233\n",
      "step: 23399, loss: 0.33887720108032227\n",
      "step: 23449, loss: 0.3674392104148865\n",
      "step: 23499, loss: 0.3963986039161682\n",
      "step: 23549, loss: 0.4245292842388153\n",
      "step: 23599, loss: 0.44448310136795044\n",
      "step: 23649, loss: 0.3623833656311035\n",
      "step: 23699, loss: 0.5478296279907227\n",
      "step: 23749, loss: 0.6184505224227905\n",
      "step: 23799, loss: 0.4167832136154175\n",
      "step: 23849, loss: 0.4031097888946533\n",
      "step: 23899, loss: 0.3925718069076538\n",
      "step: 23949, loss: 0.406319797039032\n",
      "step: 23999, loss: 0.4788469970226288\n",
      "Checkpoint is saved\n",
      "step: 24049, loss: 0.3853292465209961\n",
      "step: 24099, loss: 0.5131651163101196\n",
      "step: 24149, loss: 0.3809664845466614\n",
      "step: 24199, loss: 0.3143778443336487\n",
      "step: 24249, loss: 0.47613492608070374\n",
      "step: 24299, loss: 0.3417331576347351\n",
      "step: 24349, loss: 0.3835381269454956\n",
      "step: 24399, loss: 0.4540592133998871\n",
      "step: 24449, loss: 0.4680914282798767\n",
      "step: 24499, loss: 0.404659241437912\n",
      "step: 24549, loss: 0.5147855281829834\n",
      "step: 24599, loss: 0.40956321358680725\n",
      "step: 24649, loss: 0.36754894256591797\n",
      "step: 24699, loss: 0.46704423427581787\n",
      "step: 24749, loss: 0.38791021704673767\n",
      "step: 24799, loss: 0.39628681540489197\n",
      "step: 24849, loss: 0.3684923052787781\n",
      "step: 24899, loss: 0.31231534481048584\n",
      "step: 24949, loss: 0.4941835403442383\n",
      "step: 24999, loss: 0.5522985458374023\n",
      "Checkpoint is saved\n",
      "step: 25049, loss: 0.4919776916503906\n",
      "step: 25099, loss: 0.5888495445251465\n",
      "step: 25149, loss: 0.4035947322845459\n",
      "step: 25199, loss: 0.3602445125579834\n",
      "step: 25249, loss: 0.4608393907546997\n",
      "step: 25299, loss: 0.48346102237701416\n",
      "step: 25349, loss: 0.3662235736846924\n",
      "step: 25399, loss: 0.3290727734565735\n",
      "step: 25449, loss: 0.3114405870437622\n",
      "step: 25499, loss: 0.40800783038139343\n",
      "step: 25549, loss: 0.4014749526977539\n",
      "step: 25599, loss: 0.4663122296333313\n",
      "step: 25649, loss: 0.5877252817153931\n",
      "step: 25699, loss: 0.5021905303001404\n",
      "step: 25749, loss: 0.40784692764282227\n",
      "step: 25799, loss: 0.6153028011322021\n",
      "step: 25849, loss: 0.3789222836494446\n",
      "step: 25899, loss: 0.44600462913513184\n",
      "step: 25949, loss: 0.3002687096595764\n",
      "step: 25999, loss: 0.31728029251098633\n",
      "Checkpoint is saved\n",
      "step: 26049, loss: 0.34101948142051697\n",
      "step: 26099, loss: 0.47733074426651\n",
      "step: 26149, loss: 0.35304346680641174\n",
      "step: 26199, loss: 0.37547725439071655\n",
      "step: 26249, loss: 0.3678337633609772\n",
      "step: 26299, loss: 0.5706161260604858\n",
      "step: 26349, loss: 0.35655102133750916\n",
      "step: 26399, loss: 0.3129954934120178\n",
      "step: 26449, loss: 0.4971739649772644\n",
      "step: 26499, loss: 0.38316747546195984\n",
      "step: 26549, loss: 0.32277145981788635\n",
      "step: 26599, loss: 0.49731019139289856\n",
      "step: 26649, loss: 0.4612455666065216\n",
      "step: 26699, loss: 0.29535210132598877\n",
      "step: 26749, loss: 0.2584417760372162\n",
      "step: 26799, loss: 0.3471520245075226\n",
      "step: 26849, loss: 0.42364346981048584\n",
      "step: 26899, loss: 0.44207584857940674\n",
      "step: 26949, loss: 0.37896662950515747\n",
      "step: 26999, loss: 0.3919946551322937\n",
      "Checkpoint is saved\n",
      "step: 27049, loss: 0.44932353496551514\n",
      "step: 27099, loss: 0.4236416816711426\n",
      "step: 27149, loss: 0.43604958057403564\n",
      "step: 27199, loss: 0.33999812602996826\n",
      "step: 27249, loss: 0.36392438411712646\n",
      "step: 27299, loss: 0.39974355697631836\n",
      "step: 27349, loss: 0.36143481731414795\n",
      "step: 27399, loss: 0.40670546889305115\n",
      "step: 27449, loss: 0.3247811794281006\n",
      "step: 27499, loss: 0.3979688882827759\n",
      "step: 27549, loss: 0.45034259557724\n",
      "step: 27599, loss: 0.4191831052303314\n",
      "step: 27649, loss: 0.4366399645805359\n",
      "step: 27699, loss: 0.44586342573165894\n",
      "step: 27749, loss: 0.4987438917160034\n",
      "step: 27799, loss: 0.39388665556907654\n",
      "step: 27849, loss: 0.3661349415779114\n",
      "step: 27899, loss: 0.33290570974349976\n",
      "step: 27949, loss: 0.4998475909233093\n",
      "step: 27999, loss: 0.5157978534698486\n",
      "Checkpoint is saved\n",
      "step: 28049, loss: 0.32907459139823914\n",
      "step: 28099, loss: 0.31236106157302856\n",
      "step: 28149, loss: 0.2812555730342865\n",
      "step: 28199, loss: 0.3378415107727051\n",
      "step: 28249, loss: 0.3563295602798462\n",
      "step: 28299, loss: 0.29385191202163696\n",
      "step: 28349, loss: 0.6094088554382324\n",
      "step: 28399, loss: 0.38654789328575134\n",
      "step: 28449, loss: 0.4050036072731018\n",
      "step: 28499, loss: 0.3937048017978668\n",
      "step: 28549, loss: 0.27648454904556274\n",
      "step: 28599, loss: 0.3427492678165436\n",
      "step: 28649, loss: 0.3353269100189209\n",
      "step: 28699, loss: 0.38074883818626404\n",
      "step: 28749, loss: 0.2923780679702759\n",
      "step: 28799, loss: 0.4135417938232422\n",
      "step: 28849, loss: 0.36488258838653564\n",
      "step: 28899, loss: 0.4160562753677368\n",
      "step: 28949, loss: 0.5060471296310425\n",
      "step: 28999, loss: 0.32764995098114014\n",
      "Checkpoint is saved\n",
      "step: 29049, loss: 0.4973493814468384\n",
      "step: 29099, loss: 0.4095032513141632\n",
      "step: 29149, loss: 0.2461373507976532\n",
      "step: 29199, loss: 0.3358076512813568\n",
      "step: 29249, loss: 0.41085147857666016\n",
      "step: 29299, loss: 0.5408177375793457\n",
      "step: 29349, loss: 0.2794526517391205\n",
      "step: 29399, loss: 0.6725751757621765\n",
      "step: 29449, loss: 0.3024822175502777\n",
      "step: 29499, loss: 0.3345317244529724\n",
      "step: 29549, loss: 0.5124143958091736\n",
      "step: 29599, loss: 0.23539525270462036\n",
      "step: 29649, loss: 0.38149940967559814\n",
      "step: 29699, loss: 0.3103836178779602\n",
      "step: 29749, loss: 0.31622743606567383\n",
      "step: 29799, loss: 0.3433471918106079\n",
      "step: 29849, loss: 0.3910492956638336\n",
      "step: 29899, loss: 0.35230863094329834\n",
      "step: 29949, loss: 0.357150673866272\n",
      "step: 29999, loss: 0.35706961154937744\n",
      "Checkpoint is saved\n",
      "step: 30049, loss: 0.38051629066467285\n",
      "step: 30099, loss: 0.3747800886631012\n",
      "step: 30149, loss: 0.2555593252182007\n",
      "step: 30199, loss: 0.4333235025405884\n",
      "step: 30249, loss: 0.37737417221069336\n",
      "step: 30299, loss: 0.4415428042411804\n",
      "step: 30349, loss: 0.35485219955444336\n",
      "step: 30399, loss: 0.3202779293060303\n",
      "step: 30449, loss: 0.17467927932739258\n",
      "step: 30499, loss: 0.45707616209983826\n",
      "step: 30549, loss: 0.31692880392074585\n",
      "step: 30599, loss: 0.4785444736480713\n",
      "step: 30649, loss: 0.41832205653190613\n",
      "step: 30699, loss: 0.4281790256500244\n",
      "step: 30749, loss: 0.5200564861297607\n",
      "step: 30799, loss: 0.3136334717273712\n",
      "step: 30849, loss: 1.0540175437927246\n",
      "step: 30899, loss: 0.47263187170028687\n",
      "step: 30949, loss: 0.3273034989833832\n",
      "step: 30999, loss: 0.24618571996688843\n",
      "Checkpoint is saved\n",
      "step: 31049, loss: 0.3176347613334656\n",
      "step: 31099, loss: 0.37750592827796936\n",
      "step: 31149, loss: 0.4190818965435028\n",
      "step: 31199, loss: 0.3844345808029175\n",
      "step: 31249, loss: 0.23741549253463745\n",
      "step: 31299, loss: 0.35601043701171875\n",
      "step: 31349, loss: 0.38686463236808777\n",
      "step: 31399, loss: 2.692992687225342\n",
      "step: 31449, loss: 0.4156267046928406\n",
      "step: 31499, loss: 0.5106425881385803\n",
      "step: 31549, loss: 0.4313942492008209\n",
      "step: 31599, loss: 0.3129408657550812\n",
      "step: 31649, loss: 0.23813393712043762\n",
      "step: 31699, loss: 0.393400639295578\n",
      "step: 31749, loss: 0.3742496371269226\n",
      "step: 31799, loss: 0.27439677715301514\n",
      "step: 31849, loss: 0.33267390727996826\n",
      "step: 31899, loss: 0.3579072952270508\n",
      "step: 31949, loss: 0.3248934745788574\n",
      "step: 31999, loss: 0.19332420825958252\n",
      "Checkpoint is saved\n",
      "step: 32049, loss: 0.2409954071044922\n",
      "step: 32099, loss: 0.3019339442253113\n",
      "step: 32149, loss: 0.36515578627586365\n",
      "step: 32199, loss: 0.3022712469100952\n",
      "step: 32249, loss: 0.3092145323753357\n",
      "step: 32299, loss: 0.2593488097190857\n",
      "step: 32349, loss: 0.2885671854019165\n",
      "step: 32399, loss: 0.41156283020973206\n",
      "step: 32449, loss: 0.20410415530204773\n",
      "step: 32499, loss: 0.3516041934490204\n",
      "step: 32549, loss: 0.4205106198787689\n",
      "step: 32599, loss: 0.2908664345741272\n",
      "step: 32649, loss: 0.3247324228286743\n",
      "step: 32699, loss: 0.3784988224506378\n",
      "step: 32749, loss: 0.3112454414367676\n",
      "step: 32799, loss: 0.3987671732902527\n",
      "step: 32849, loss: 0.18143592774868011\n",
      "step: 32899, loss: 0.3360588550567627\n",
      "step: 32949, loss: 0.27634450793266296\n",
      "step: 32999, loss: 0.24703624844551086\n",
      "Checkpoint is saved\n",
      "step: 33049, loss: 0.34914883971214294\n",
      "step: 33099, loss: 0.4362718462944031\n",
      "step: 33149, loss: 0.3480333685874939\n",
      "step: 33199, loss: 0.35671311616897583\n",
      "step: 33249, loss: 0.23271304368972778\n",
      "step: 33299, loss: 0.35079216957092285\n",
      "step: 33349, loss: 0.33658766746520996\n",
      "step: 33399, loss: 0.2611404061317444\n",
      "step: 33449, loss: 0.2700936496257782\n",
      "step: 33499, loss: 0.16820746660232544\n",
      "step: 33549, loss: 0.3203973174095154\n",
      "step: 33599, loss: 0.35815298557281494\n",
      "step: 33649, loss: 0.4111129641532898\n",
      "step: 33699, loss: 0.3136807680130005\n",
      "step: 33749, loss: 0.31025514006614685\n",
      "step: 33799, loss: 0.32225286960601807\n",
      "step: 33849, loss: 0.3720138370990753\n",
      "step: 33899, loss: 0.2351037561893463\n",
      "step: 33949, loss: 0.4958479404449463\n",
      "step: 33999, loss: 0.3993798494338989\n",
      "Checkpoint is saved\n",
      "step: 34049, loss: 0.2636178731918335\n",
      "step: 34099, loss: 0.367110013961792\n",
      "step: 34149, loss: 0.2747867703437805\n",
      "step: 34199, loss: 0.2907709777355194\n",
      "step: 34249, loss: 0.16956794261932373\n",
      "step: 34299, loss: 0.27853822708129883\n",
      "step: 34349, loss: 0.33867835998535156\n",
      "step: 34399, loss: 0.28481993079185486\n",
      "step: 34449, loss: 0.3986107409000397\n",
      "step: 34499, loss: 0.2930554747581482\n",
      "step: 34549, loss: 0.3637514114379883\n",
      "step: 34599, loss: 0.26144322752952576\n",
      "step: 34649, loss: 0.38271278142929077\n",
      "step: 34699, loss: 0.37752512097358704\n",
      "step: 34749, loss: 0.21549522876739502\n",
      "step: 34799, loss: 0.42047643661499023\n",
      "step: 34849, loss: 0.22703593969345093\n",
      "step: 34899, loss: 0.2806779742240906\n",
      "step: 34949, loss: 0.6043823957443237\n",
      "step: 34999, loss: 0.3210314214229584\n",
      "Checkpoint is saved\n",
      "step: 35049, loss: 0.46879613399505615\n",
      "step: 35099, loss: 0.28743141889572144\n",
      "step: 35149, loss: 0.35669708251953125\n",
      "step: 35199, loss: 0.33277931809425354\n",
      "step: 35249, loss: 0.3232443928718567\n",
      "step: 35299, loss: 0.2469954639673233\n",
      "step: 35349, loss: 0.36612653732299805\n",
      "step: 35399, loss: 0.455447256565094\n",
      "step: 35449, loss: 0.3371570110321045\n",
      "step: 35499, loss: 0.6083540320396423\n",
      "step: 35549, loss: 0.4800376296043396\n",
      "step: 35599, loss: 0.2527138590812683\n",
      "step: 35649, loss: 0.35344797372817993\n",
      "step: 35699, loss: 0.3026546239852905\n",
      "step: 35749, loss: 0.3674527406692505\n",
      "step: 35799, loss: 0.29293447732925415\n",
      "step: 35849, loss: 0.4555307924747467\n",
      "step: 35899, loss: 0.27659761905670166\n",
      "step: 35949, loss: 0.2682493329048157\n",
      "step: 35999, loss: 0.31733381748199463\n",
      "Checkpoint is saved\n",
      "step: 36049, loss: 0.3520139157772064\n",
      "step: 36099, loss: 0.38561683893203735\n",
      "step: 36149, loss: 0.319924533367157\n",
      "step: 36199, loss: 0.2554771900177002\n",
      "step: 36249, loss: 0.4719266891479492\n",
      "step: 36299, loss: 0.46080201864242554\n",
      "step: 36349, loss: 0.42398232221603394\n",
      "step: 36399, loss: 0.5024734139442444\n",
      "step: 36449, loss: 0.22927148640155792\n",
      "step: 36499, loss: 0.2583153545856476\n",
      "step: 36549, loss: 0.3378000557422638\n",
      "step: 36599, loss: 0.3087940812110901\n",
      "step: 36649, loss: 0.33803871273994446\n",
      "step: 36699, loss: 0.5089812874794006\n",
      "step: 36749, loss: 0.38118231296539307\n",
      "step: 36799, loss: 0.38937249779701233\n",
      "step: 36849, loss: 0.3563622534275055\n",
      "step: 36899, loss: 0.2709217071533203\n",
      "step: 36949, loss: 0.3034515380859375\n",
      "step: 36999, loss: 0.24175621569156647\n",
      "Checkpoint is saved\n",
      "step: 37049, loss: 0.335355281829834\n",
      "step: 37099, loss: 0.2471843957901001\n",
      "step: 37149, loss: 0.24851582944393158\n",
      "step: 37199, loss: 0.3234654664993286\n",
      "step: 37249, loss: 0.3267238736152649\n",
      "step: 37299, loss: 0.29851800203323364\n",
      "step: 37349, loss: 0.2549794316291809\n",
      "step: 37399, loss: 0.2843278646469116\n",
      "step: 37449, loss: 0.29172104597091675\n",
      "step: 37499, loss: 0.34356456995010376\n",
      "step: 37549, loss: 0.28337591886520386\n",
      "step: 37599, loss: 0.32744768261909485\n",
      "step: 37649, loss: 0.2709795832633972\n",
      "step: 37699, loss: 0.22549623250961304\n",
      "step: 37749, loss: 0.33042991161346436\n",
      "step: 37799, loss: 0.20169639587402344\n",
      "step: 37849, loss: 0.17000263929367065\n",
      "step: 37899, loss: 0.3446657061576843\n",
      "step: 37949, loss: 0.2724173665046692\n",
      "step: 37999, loss: 0.3599739670753479\n",
      "Checkpoint is saved\n",
      "step: 38049, loss: 0.313561350107193\n",
      "step: 38099, loss: 0.2551696300506592\n",
      "step: 38149, loss: 0.25591641664505005\n",
      "step: 38199, loss: 0.2820611000061035\n",
      "step: 38249, loss: 0.6690939664840698\n",
      "step: 38299, loss: 0.38211315870285034\n",
      "step: 38349, loss: 0.33936750888824463\n",
      "step: 38399, loss: 0.2709943652153015\n",
      "step: 38449, loss: 0.24889731407165527\n",
      "step: 38499, loss: 0.20897626876831055\n",
      "step: 38549, loss: 0.31496623158454895\n",
      "step: 38599, loss: 0.3365512192249298\n",
      "step: 38649, loss: 0.3748318552970886\n",
      "step: 38699, loss: 0.64985191822052\n",
      "step: 38749, loss: 0.3828202486038208\n",
      "step: 38799, loss: 0.2466142177581787\n",
      "step: 38849, loss: 0.31857621669769287\n",
      "step: 38899, loss: 0.3594016432762146\n",
      "step: 38949, loss: 0.27706873416900635\n",
      "step: 38999, loss: 0.24043340981006622\n",
      "Checkpoint is saved\n",
      "step: 39049, loss: 0.3061348497867584\n",
      "step: 39099, loss: 0.203656405210495\n",
      "step: 39149, loss: 0.31120169162750244\n",
      "step: 39199, loss: 0.3725631833076477\n",
      "step: 39249, loss: 0.39154815673828125\n",
      "step: 39299, loss: 0.28102365136146545\n",
      "step: 39349, loss: 0.4030805826187134\n",
      "step: 39399, loss: 0.3726380169391632\n",
      "step: 39449, loss: 0.2704058289527893\n",
      "step: 39499, loss: 0.22469502687454224\n",
      "step: 39549, loss: 0.16629467904567719\n",
      "step: 39599, loss: 0.25267934799194336\n",
      "step: 39649, loss: 0.2090490460395813\n",
      "step: 39699, loss: 0.23046168684959412\n",
      "step: 39749, loss: 0.3282153010368347\n",
      "step: 39799, loss: 0.41917192935943604\n",
      "step: 39849, loss: 0.21713759005069733\n",
      "step: 39899, loss: 0.4014623761177063\n",
      "step: 39949, loss: 0.3468170762062073\n",
      "step: 39999, loss: 0.30473142862319946\n",
      "Checkpoint is saved\n",
      "Training time for 40000 steps: 4550.093817234039s\n"
     ]
    }
   ],
   "source": [
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 50 == 49 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 1000 == 999:\n",
    "            saver.save(sess, './checkpoints/', global_step=step)\n",
    "            print('Checkpoint is saved')\n",
    "            \n",
    "    print('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAGTCAYAAAAfltc1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJvT/CD13dkuzv/YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f97d960b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_test = {'X':X_test, 'Y':Y_test}\n",
    "pickle.dump(encoded_test, open(\"./encoded_test.p\", mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_location = \"./data.p\"\n",
    "X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 20\n",
    "output_seq_len = 22\n",
    "l1_vocab_size = len(l1_vocab) + 2 # + <pad>, <ukn>\n",
    "l2_vocab_size = len(l2_vocab) + 4 # + <pad>, <ukn>, <eos>, <go>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes])\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes])\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = l2_word2idx['<pad>'])\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == l2_word2idx['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(l2_idx2word[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_padding(x, l1_word2idx, length = 20):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [l1_word2idx['<pad>']]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_model(sentences):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # read dataset\n",
    "    dataset_location = \"./data.p\"\n",
    "    X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab = read_dataset(dataset_location)\n",
    "    \n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # placeholders\n",
    "        encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "        decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "        # output projection\n",
    "        size = 512\n",
    "        w_t = tf.get_variable('proj_w', [l2_vocab_size, size], tf.float32)\n",
    "        b = tf.get_variable('proj_b', [l2_vocab_size], tf.float32)\n",
    "        w = tf.transpose(w_t)\n",
    "        output_projection = (w, b)\n",
    "\n",
    "\n",
    "        # change the model so that output at time t can be fed as input at time t+1\n",
    "        outputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                                                    encoder_inputs,\n",
    "                                                    decoder_inputs,\n",
    "                                                    tf.nn.rnn_cell.BasicLSTMCell(size),\n",
    "                                                    num_encoder_symbols = l1_vocab_size,\n",
    "                                                    num_decoder_symbols = l2_vocab_size,\n",
    "                                                    embedding_size = 80,\n",
    "                                                    feed_previous = True, # <-----this is changed----->\n",
    "                                                    output_projection = output_projection,\n",
    "                                                    dtype = tf.float32)\n",
    "\n",
    "        # ops for projecting outputs\n",
    "        outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "        \n",
    "        sentences = [[l1_word2idx.get(word.strip(',.\" ;:)(|][?!<>'), 0) for word in sentence.split(' ')] for sentence in sentences]\n",
    "        \n",
    "        encoded_sentences = data_padding(sentences, l1_word2idx)\n",
    "        \n",
    "        \n",
    "        # restore all variables - use the last checkpoint saved\n",
    "        saver = tf.train.Saver()\n",
    "        path = tf.train.latest_checkpoint('./checkpoints/')\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # restore\n",
    "            saver.restore(sess, path)\n",
    "\n",
    "            # feed data into placeholders\n",
    "            feed = {}\n",
    "            for i in range(input_seq_len):\n",
    "                feed[encoder_inputs[i].name] = np.array([encoded_sentences[j][i] for j in range(len(encoded_sentences))])\n",
    "            feed[decoder_inputs[0].name] = np.array([l2_word2idx['<go>']] * len(encoded_sentences))\n",
    "            \n",
    "            # translate\n",
    "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "            \n",
    "            for i in range(len(encoded_sentences)):\n",
    "                \n",
    "                ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "                \n",
    "                #decode output sequence\n",
    "                words = decode_output(ouput_seq)\n",
    "                \n",
    "                temp = \"\"\"\"\"\"\n",
    "                for i in range(len(words)):\n",
    "                    if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                        temp += words[i] + \" \"\n",
    "                result.append(temp.strip())\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is your name', 'who are these', 'tomorrow tomorrow hai', 'kids love up comic', 'water <ukn>', 'dance together notes something']\n"
     ]
    }
   ],
   "source": [
    "test = [ \"आपका नाम क्या है\", \"यह कौन है\", \"कल शनिवार है\", \"बच्चे पानी प्यार करते हैं\", \"पानी पिएं\", \"अधिक नृत्य करें\"]\n",
    "result = translate_model(test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sentences(filepath):\n",
    "    sentences = []\n",
    "    with codecs.open(filepath, encoding=\"utf-8\", mode=\"r\") as fp:\n",
    "        for sentence in fp:\n",
    "            sentences.append(sentence.lower())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sentences_format2(filepath):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with codecs.open(filepath, encoding = \"utf-8\", mode = \"r\") as fp:\n",
    "        for sentence in fp:\n",
    "            splits = sentence.split(\"\\t\")\n",
    "            X.append(splits[3].strip().lower())\n",
    "            Y.append(splits[4].strip().lower())\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(l1_sentences, l2_sentences):\n",
    "    \"\"\"\n",
    "    Need to exchange strip functionality for hindi and english\n",
    "    \n",
    "    String for Hindi   => ',.\" ;:)(|][?!<>a-zA-Z'\n",
    "    String for English => ',.\" ;:)(][?!-\\''\n",
    "    \"\"\"\n",
    "    l1_vocab_dict = Counter(word.strip(',.\" ;:)(|][?!<>') for sentence in l1_sentences for word in sentence.split())\n",
    "    l2_vocab_dict = Counter(word.strip(',.\" ;:)(][?!-\\'') for sentence in l2_sentences for word in sentence.split())\n",
    "    #l1_vocab_dict = Counter(word for sentence in l1_sentences for word in sentence.split())\n",
    "    #l2_vocab_dict = Counter(word for sentence in l2_sentences for word in sentence.split())\n",
    "    \n",
    "    l1_vocab = list(map(lambda x: x[0], sorted(l1_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    l2_vocab = list(map(lambda x: x[0], sorted(l2_vocab_dict.items(), key = lambda x: -x[1])))\n",
    "    \n",
    "    # Limit the vocabulary size. Consider only the top 20,000 and 30,000 words respectively\n",
    "    l1_vocab = l1_vocab[:30000]\n",
    "    l2_vocab = l2_vocab[:30000]\n",
    "    \n",
    "    # Build a Word to Index Dictionary for English\n",
    "    start_idx = 2\n",
    "    l1_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(l1_vocab)])\n",
    "    l1_word2idx['<ukn>'] = 0 # Unknown words\n",
    "    l1_word2idx['<pad>'] = 1 # Padding word\n",
    "    \n",
    "    # Build an Index to Word Dictionary for English using the already created Word to Index Dictionary\n",
    "    l1_idx2word = dict([(idx, word) for word, idx in l1_word2idx.items()])\n",
    "    \n",
    "    # Build a Word to Index Dictionary for Hindi\n",
    "    start_idx = 4\n",
    "    l2_word2idx = dict([(word, idx+start_idx) for idx, word in enumerate(l2_vocab)])\n",
    "    l2_word2idx['<ukn>'] = 0 # Unknown\n",
    "    l2_word2idx['<go>']  = 1 \n",
    "    l2_word2idx['<eos>'] = 2 # End of sentence\n",
    "    l2_word2idx['<pad>'] = 3 # Padding\n",
    "    \n",
    "    # Build an Index to Word Dictionary for Hindi using the already created Word to Index Dictionary\n",
    "    l2_idx2word = dict([(idx, word) for word, idx in l2_word2idx.items()])\n",
    "\n",
    "    # Encode words in senteces by their index in Vocabulary\n",
    "    x = [[l1_word2idx.get(word.strip(',.\" ;:)(|][?!<>'), 0) for word in sentence.split()] for sentence in l1_sentences]\n",
    "    y = [[l2_word2idx.get(word.strip(',.\" ;:)(][?!-\\''), 0) for word in sentence.split()] for sentence in l2_sentences]\n",
    "    #x = [[l1_word2idx.get(word, 0) for word in sentence.split()] for sentence in l1_sentences]\n",
    "    #y = [[l2_word2idx.get(word, 0) for word in sentence.split()] for sentence in l2_sentences]\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2\n",
    "        if abs(n1 - n2) < 0.3 * n:\n",
    "            if n1 <= 20 and n2 <= 20:\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "    \n",
    "    return X, Y, l1_word2idx, l1_idx2word, l1_vocab, l2_word2idx, l2_idx2word, l2_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_dataset(filepath, obj):\n",
    "    with open(filepath, 'wb') as fp:\n",
    "        pickle.dump(obj, fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_save_location = \"./data.p\"\n",
    "    \n",
    "    X, Y = read_sentences_format2(\"./hindencorp05.plaintext\")\n",
    "    # Here, X represents English and Y Represents Hindi. \n",
    "    \"\"\"\n",
    "    For English to Hindi, \n",
    "    l1_sentences = X\n",
    "    l2_sentences = Y\n",
    "    \"\"\"\n",
    "    # This \n",
    "    l1_sentences = Y\n",
    "    l2_sentences = X\n",
    "    save_dataset(dataset_save_location , create_dataset(l1_sentences, l2_sentences))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sharaabi', 'politicians do not have permission to do what needs to be done.']\n",
      "['शराबी', 'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .']\n"
     ]
    }
   ],
   "source": [
    "X, Y = read_sentences_format2(\"./hindencorp05.plaintext\")\n",
    "print(X[:2])\n",
    "print(Y[:2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
